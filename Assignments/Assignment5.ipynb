{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0kZxPnpsiNT"
      },
      "source": [
        "**Step 0 - import NN libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od3Qre5L1Oxc"
      },
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "# Commonly used modules\n",
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "\n",
        "# Images, plots, display, and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtrdMz_AspOn"
      },
      "source": [
        "**Step 1 - Load the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJk90es8szd5",
        "outputId": "d09e93ce-7636-40fd-dead-86b57f8c1608"
      },
      "source": [
        "# Loading data from keras datasets\n",
        "data = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
        "print(data)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((array([[[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "       [[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "       [[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "       [[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "       [[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8), array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)), (array([[[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "       [[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "       [[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "       [[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "       [[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8), array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeFEdXZWtGmB"
      },
      "source": [
        "**Step 2 - Data pre-processing & Step 3 - Divide dataset into Train/Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqT3C7sondBT"
      },
      "source": [
        "# Spliting train and test data from datasets\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "pFrzUyR4ZT-O",
        "outputId": "892532b7-afd9-4425-8dfd-9b47dece18aa"
      },
      "source": [
        "# plotting train data \n",
        "fig = plt.figure()\n",
        "for i in range(9):\n",
        "  plt.subplot(3,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(x_train[i], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Digit: {}\".format(y_train[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "fig"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "wg7JRSyznjRX",
        "outputId": "8b181319-3040-4efa-c215-59492ca36d05"
      },
      "source": [
        "# Visualizing first data\n",
        "fig = plt.figure()\n",
        "plt.subplot(2,1,1)\n",
        "plt.imshow(x_train[0], cmap='gray', interpolation='none')\n",
        "plt.title(\"Digit: {}\".format(y_train[0]))\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.subplot(2,1,2)\n",
        "plt.hist(x_train[0].reshape(784))\n",
        "plt.title(\"Pixel Value Distribution\")\n",
        "fig"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9jXyyaMnywf",
        "outputId": "4e78c3ec-ff34-4977-aa55-194aee7e806a"
      },
      "source": [
        "# let's print the shape before we reshape and normalize\n",
        "print(\"X_train shape\", x_train.shape)\n",
        "print(\"y_train shape\", y_train.shape)\n",
        "print(\"X_test shape\", x_test.shape)\n",
        "print(\"y_test shape\", y_test.shape)\n",
        "\n",
        "# building the input vector from the 28x28 pixels\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# normalizing the data to help with the training\n",
        "x_train_flattened = x_train.reshape(len(x_train), 28*28)\n",
        "x_test_flattened = x_test.reshape(len(x_test), 28*28)\n",
        "\n",
        "# print the final input shape ready for training\n",
        "print(\"Train matrix shape\", x_train.shape)\n",
        "print(\"Test matrix shape\", x_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape (60000, 28, 28)\n",
            "y_train shape (60000,)\n",
            "X_test shape (10000, 28, 28)\n",
            "y_test shape (10000,)\n",
            "Train matrix shape (60000, 784)\n",
            "Test matrix shape (10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvZrUKzgn6WP",
        "outputId": "14dbb0d1-b1a0-44bb-da93-95da97339451"
      },
      "source": [
        "# Checking for unique value\n",
        "print(np.unique(y_train, return_counts=True))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMKm8_g6n9dM",
        "outputId": "a88681e4-0833-44a1-e9a6-d8380f63bec3"
      },
      "source": [
        "# one-hot encoding using keras' numpy-related utilities\n",
        "n_classes = 10\n",
        "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
        "\n",
        "# One hot encoding for categorical target labels \n",
        "Y_train = keras.utils.to_categorical(y_train)\n",
        "Y_test = keras.utils.to_categorical(y_test)\n",
        "\n",
        "print(\"Shape after one-hot encoding: \", Y_train.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before one-hot encoding:  (60000,)\n",
            "Shape after one-hot encoding:  (60000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvJ7TZx1teKl",
        "outputId": "ba0b3a54-aedb-452e-b789-aa8bd6b84956"
      },
      "source": [
        "print(x_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5rMqpxithjW",
        "outputId": "5cb463fe-2636-402f-edbc-805e0aeb0007"
      },
      "source": [
        "print(Y_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO5gRq3DtmLe",
        "outputId": "f4b3c2aa-32dc-40cf-8625-0112d0070f63"
      },
      "source": [
        "print(x_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ba2dbTdtrNa",
        "outputId": "8ee49a44-b7ef-4de8-c215-0205ef1311aa"
      },
      "source": [
        "print(Y_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x97c0wetz9g",
        "outputId": "c1244234-d2d7-4de1-8bbe-c35c89c4ecc7"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cxUTfbwt9PZ",
        "outputId": "ef3d1dc9-026b-4761-b86f-ef66cc341e96"
      },
      "source": [
        "Y_train.shape\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alXMMmlft-oO",
        "outputId": "3e30ed8c-349b-422e-b67f-24f5efa6c86e"
      },
      "source": [
        "x_test.shape\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flu6KXr6uARj",
        "outputId": "5e5361a7-6d13-41a0-95ec-c36a0b24c47c"
      },
      "source": [
        "Y_test.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpvmTi1mww7y"
      },
      "source": [
        "**Step 4 - Build a simple dense network, use ExponentialLearningRate()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb2q7N8dwxNu"
      },
      "source": [
        "# Creating simple dense network\n",
        "def create_model(): \n",
        "    model = Sequential([Flatten(input_shape=(28, 28)),\n",
        "        Dense(300, activation='relu'),\n",
        "        Dense(100, activation='relu'),\n",
        "        Dense(10, activation='softmax')])\n",
        "    return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7C41WFa1Mmk"
      },
      "source": [
        "# Defining function for plotting learning rate\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "\n",
        "def plot_lr(history):\n",
        "    learning_rate = history.history['lr']\n",
        "    epochs = range(1, len(learning_rate) + 1)\n",
        "    plt.plot(epochs, learning_rate)\n",
        "    plt.title('Learning rate')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Learning rate')\n",
        "    plt.show()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb0Oe4kC1Psn"
      },
      "source": [
        "# Defining function for plotting graph based on metric\n",
        "\n",
        "def plot_metric(history, metric):\n",
        "    train_metrics = history.history[metric]\n",
        "    val_metrics = history.history['val_'+metric]\n",
        "    epochs = range(1, len(train_metrics) + 1)\n",
        "    plt.plot(epochs, train_metrics)\n",
        "    plt.plot(epochs, val_metrics)\n",
        "    plt.title('Training and validation '+ metric)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
        "    plt.show()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w3xMvF4JzkE"
      },
      "source": [
        "# Defining function for learning rate vs loss\n",
        "\n",
        "def plot_metric2(history, metric):\n",
        "    train_metrics = history.history[metric]\n",
        "    learning_rate = history.history['lr']\n",
        "    val_metrics = history.history['val_'+metric]\n",
        "    #epochs = range(1, len(train_metrics) + 1)\n",
        "    plt.plot(learning_rate, train_metrics)\n",
        "    plt.plot(learning_rate, val_metrics)\n",
        "    plt.title('Training and validation learning rate vs loss '+ metric)\n",
        "    plt.xlabel(\"Learning_rate\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
        "    plt.show()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LP-3CWZVxEUw",
        "outputId": "13894de2-16bc-48ec-b44c-6404b149178f"
      },
      "source": [
        "# Creating first simple dense model with sgd\n",
        "\n",
        "model = create_model()\n",
        "model.compile(optimizer='sgd', # Learning rate defaults to 0.01\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tGQ_EsKxRUL",
        "outputId": "fd673cb8-2c01-4bf9-dcd6-e72ff313ba9e"
      },
      "source": [
        "# Training model\n",
        "\n",
        "initial_learning_rate = 0.01\n",
        "def lr_exp_decay(epoch, lr):\n",
        "    k = 0.1\n",
        "    return initial_learning_rate * math.exp(-k*epoch)\n",
        "# Fit the model to the training data\n",
        "history_exp_decay = model.fit(\n",
        "    x_train, \n",
        "    Y_train, \n",
        "    epochs=100, \n",
        "    validation_split=0.2,\n",
        "    batch_size=64,\n",
        "    callbacks=[keras.callbacks.LearningRateScheduler(lr_exp_decay, verbose=1)],\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.01.\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='flatten_1_input'), name='flatten_1_input', description=\"created by layer 'flatten_1_input'\"), but it was called on an input with incompatible shape (64, 784).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='flatten_1_input'), name='flatten_1_input', description=\"created by layer 'flatten_1_input'\"), but it was called on an input with incompatible shape (64, 784).\n",
            "748/750 [============================>.] - ETA: 0s - loss: 31145254912.0000 - accuracy: 0.1139WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='flatten_1_input'), name='flatten_1_input', description=\"created by layer 'flatten_1_input'\"), but it was called on an input with incompatible shape (None, 784).\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 31062202368.0000 - accuracy: 0.1138 - val_loss: 2.4252 - val_accuracy: 0.1060\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.009048374180359595.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3012 - accuracy: 0.1140 - val_loss: 2.4251 - val_accuracy: 0.1060\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.008187307530779819.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3011 - accuracy: 0.1140 - val_loss: 2.4252 - val_accuracy: 0.1060\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.007408182206817179.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4252 - val_accuracy: 0.1060\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.006703200460356393.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4252 - val_accuracy: 0.1060\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.006065306597126334.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4252 - val_accuracy: 0.1060\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.005488116360940264.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4252 - val_accuracy: 0.1060\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.004965853037914095.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4252 - val_accuracy: 0.1060\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.004493289641172216.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4252 - val_accuracy: 0.1060\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.004065696597405992.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0036787944117144234.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.003328710836980796.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0030119421191220205.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.002725317930340126.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0024659696394160645.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0022313016014842983.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.002018965179946554.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.001826835240527346.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0016529888822158654.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0014956861922263505.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0013533528323661271.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.001224564282529819.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0011080315836233387.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.001002588437228037.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 25/100\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0009071795328941247.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0008208499862389881.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0007427357821433388.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0006720551273974976.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0006081006262521795.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0005502322005640721.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0004978706836786395.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.000450492023935578.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0004076220397836621.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.00036883167401239995.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0003337326996032607.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.00030197383422318503.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002732372244729256.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.00024723526470339386.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.00022370771856165591.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002024191144580438.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 0.00018315638888734178.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 0.00016572675401761236.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 0.00014995576820477704.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 0.00013568559012200934.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 0.00012277339903068437.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 0.00011108996538242307.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 0.00010051835744633576.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 9.095277101695816e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 49/100\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 8.229747049020023e-05.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 7.446583070924338e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 00051: LearningRateScheduler setting learning rate to 6.737946999085467e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 52/100\n",
            "\n",
            "Epoch 00052: LearningRateScheduler setting learning rate to 6.096746565515633e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 00053: LearningRateScheduler setting learning rate to 5.5165644207607714e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 00054: LearningRateScheduler setting learning rate to 4.991593906910213e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 00055: LearningRateScheduler setting learning rate to 4.516580942612666e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 00056: LearningRateScheduler setting learning rate to 4.086771438464067e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 57/100\n",
            "\n",
            "Epoch 00057: LearningRateScheduler setting learning rate to 3.697863716482929e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 58/100\n",
            "\n",
            "Epoch 00058: LearningRateScheduler setting learning rate to 3.3459654574712724e-05.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 00059: LearningRateScheduler setting learning rate to 3.027554745375813e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 60/100\n",
            "\n",
            "Epoch 00060: LearningRateScheduler setting learning rate to 2.7394448187683685e-05.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 00061: LearningRateScheduler setting learning rate to 2.4787521766663585e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 00062: LearningRateScheduler setting learning rate to 2.2428677194858013e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 00063: LearningRateScheduler setting learning rate to 2.029430636295734e-05.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 00064: LearningRateScheduler setting learning rate to 1.8363047770289057e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 00065: LearningRateScheduler setting learning rate to 1.661557273173934e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 66/100\n",
            "\n",
            "Epoch 00066: LearningRateScheduler setting learning rate to 1.5034391929775723e-05.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 00067: LearningRateScheduler setting learning rate to 1.3603680375478928e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 68/100\n",
            "\n",
            "Epoch 00068: LearningRateScheduler setting learning rate to 1.230911902673481e-05.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 00069: LearningRateScheduler setting learning rate to 1.1137751478448024e-05.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 00070: LearningRateScheduler setting learning rate to 1.0077854290485104e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 00071: LearningRateScheduler setting learning rate to 9.118819655545163e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 00072: LearningRateScheduler setting learning rate to 8.25104923265904e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 00073: LearningRateScheduler setting learning rate to 7.465858083766792e-06.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 00074: LearningRateScheduler setting learning rate to 6.755387751938438e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 00075: LearningRateScheduler setting learning rate to 6.112527611295723e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 00076: LearningRateScheduler setting learning rate to 5.530843701478336e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 00077: LearningRateScheduler setting learning rate to 5.004514334406104e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 00078: LearningRateScheduler setting learning rate to 4.528271828867969e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 00079: LearningRateScheduler setting learning rate to 4.097349789797864e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 00080: LearningRateScheduler setting learning rate to 3.707435404590882e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 00081: LearningRateScheduler setting learning rate to 3.3546262790251185e-06.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 00082: LearningRateScheduler setting learning rate to 3.035391380788668e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 00083: LearningRateScheduler setting learning rate to 2.7465356997214204e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 00084: LearningRateScheduler setting learning rate to 2.4851682710795183e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 00085: LearningRateScheduler setting learning rate to 2.248673241788482e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 00086: LearningRateScheduler setting learning rate to 2.0346836901064417e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 00087: LearningRateScheduler setting learning rate to 1.841057936675792e-06.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 00088: LearningRateScheduler setting learning rate to 1.6658581098763324e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 00089: LearningRateScheduler setting learning rate to 1.507330750954765e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 00090: LearningRateScheduler setting learning rate to 1.363889264820114e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 00091: LearningRateScheduler setting learning rate to 1.2340980408667956e-06.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 00092: LearningRateScheduler setting learning rate to 1.1166580849011478e-06.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 00093: LearningRateScheduler setting learning rate to 1.0103940183709325e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 00094: LearningRateScheduler setting learning rate to 9.142423147817327e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 00095: LearningRateScheduler setting learning rate to 8.272406555663223e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 00096: LearningRateScheduler setting learning rate to 7.48518298877006e-07.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 00097: LearningRateScheduler setting learning rate to 6.772873649085378e-07.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 00098: LearningRateScheduler setting learning rate to 6.128349505322203e-07.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 00099: LearningRateScheduler setting learning rate to 5.545159943217694e-07.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 00100: LearningRateScheduler setting learning rate to 5.017468205617528e-07.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.4253 - val_accuracy: 0.1060\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "JIY2U57S0yaG",
        "outputId": "874a55dc-f13c-455c-8038-dbc20002984b"
      },
      "source": [
        "plot_lr(history_exp_decay)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 398.50625 277.314375\" width=\"398.50625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 398.50625 277.314375 \nL 398.50625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 56.50625 239.758125 \nL 391.30625 239.758125 \nL 391.30625 22.318125 \nL 56.50625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"me703917265\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"68.650052\" xlink:href=\"#me703917265\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(65.468802 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.137655\" xlink:href=\"#me703917265\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(123.775155 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"191.625258\" xlink:href=\"#me703917265\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(185.262758 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"253.112862\" xlink:href=\"#me703917265\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(246.750362 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"314.600465\" xlink:href=\"#me703917265\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(308.237965 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"376.088068\" xlink:href=\"#me703917265\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(366.544318 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Epochs -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(205.990625 268.034687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"306.201172\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m8656bbdefd\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m8656bbdefd\" y=\"229.884407\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.000 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(20.878125 233.683626)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m8656bbdefd\" y=\"190.347877\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.002 -->\n      <g transform=\"translate(20.878125 194.147096)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m8656bbdefd\" y=\"150.811347\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.004 -->\n      <g transform=\"translate(20.878125 154.610566)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m8656bbdefd\" y=\"111.274817\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.006 -->\n      <g transform=\"translate(20.878125 115.074036)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m8656bbdefd\" y=\"71.738287\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.008 -->\n      <g transform=\"translate(20.878125 75.537506)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m8656bbdefd\" y=\"32.201757\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.010 -->\n      <g transform=\"translate(20.878125 36.000976)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- Learning rate -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n     </defs>\n     <g transform=\"translate(14.798437 164.49125)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"115.486328\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"176.765625\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"216.128906\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"279.507812\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"307.291016\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"370.669922\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"434.146484\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"465.933594\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"507.046875\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"568.326172\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"607.535156\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#pd009d3ce58)\" d=\"M 71.724432 32.201761 \nL 74.798812 51.013745 \nL 77.873192 68.035533 \nL 80.947572 83.437499 \nL 84.021952 97.373767 \nL 87.096333 109.983817 \nL 90.170713 121.39387 \nL 93.245093 131.71811 \nL 96.319473 141.059867 \nL 99.393853 149.512639 \nL 102.468233 157.161023 \nL 105.542614 164.08157 \nL 108.616994 170.343537 \nL 111.691374 176.009602 \nL 114.765754 181.136466 \nL 117.840134 185.775448 \nL 120.914514 189.97297 \nL 123.988895 193.771045 \nL 127.063275 197.207686 \nL 130.137655 200.317285 \nL 133.212035 203.130971 \nL 136.286415 205.676896 \nL 139.360795 207.980545 \nL 142.435176 210.064974 \nL 145.509556 211.951042 \nL 148.583936 213.657628 \nL 151.658316 215.201809 \nL 154.732696 216.599044 \nL 157.807076 217.863313 \nL 160.881457 219.007272 \nL 163.955837 220.042368 \nL 167.030217 220.978962 \nL 170.104597 221.826427 \nL 173.178977 222.593245 \nL 176.253357 223.287091 \nL 179.327738 223.914908 \nL 182.402118 224.482981 \nL 185.476498 224.996995 \nL 188.550878 225.462094 \nL 191.625258 225.882932 \nL 194.699638 226.263723 \nL 197.774019 226.608277 \nL 200.848399 226.920042 \nL 203.922779 227.202139 \nL 206.997159 227.45739 \nL 210.071539 227.688351 \nL 213.145919 227.897334 \nL 216.2203 228.086429 \nL 219.29468 228.257529 \nL 222.36906 228.412347 \nL 225.44344 228.552432 \nL 228.51782 228.679186 \nL 231.5922 228.793878 \nL 234.666581 228.897656 \nL 237.740961 228.991558 \nL 240.815341 229.076523 \nL 243.889721 229.153404 \nL 246.964101 229.222968 \nL 250.038481 229.285912 \nL 253.112862 229.342867 \nL 256.187242 229.394401 \nL 259.261622 229.441031 \nL 262.336002 229.483224 \nL 265.410382 229.521402 \nL 268.484762 229.555946 \nL 271.559143 229.587203 \nL 274.633523 229.615486 \nL 277.707903 229.641077 \nL 280.782283 229.664233 \nL 283.856663 229.685186 \nL 286.931043 229.704144 \nL 290.005424 229.721298 \nL 293.079804 229.73682 \nL 296.154184 229.750865 \nL 299.228564 229.763573 \nL 302.302944 229.775072 \nL 305.377324 229.785477 \nL 308.451705 229.794891 \nL 311.526085 229.80341 \nL 314.600465 229.811118 \nL 317.674845 229.818092 \nL 320.749225 229.824403 \nL 323.823605 229.830113 \nL 326.897986 229.83528 \nL 329.972366 229.839955 \nL 333.046746 229.844185 \nL 336.121126 229.848013 \nL 339.195506 229.851476 \nL 342.269886 229.85461 \nL 345.344267 229.857446 \nL 348.418647 229.860011 \nL 351.493027 229.862333 \nL 354.567407 229.864434 \nL 357.641787 229.866334 \nL 360.716167 229.868054 \nL 363.790548 229.86961 \nL 366.864928 229.871019 \nL 369.939308 229.872293 \nL 373.013688 229.873445 \nL 376.088068 229.874489 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 56.50625 239.758125 \nL 56.50625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 391.30625 239.758125 \nL 391.30625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 56.50625 239.758125 \nL 391.30625 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 56.50625 22.318125 \nL 391.30625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_15\">\n    <!-- Learning rate -->\n    <g transform=\"translate(183.7625 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"53.962891\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"115.486328\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"176.765625\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"216.128906\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"279.507812\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"307.291016\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"370.669922\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"434.146484\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"465.933594\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"507.046875\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"568.326172\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"607.535156\" xlink:href=\"#DejaVuSans-101\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pd009d3ce58\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"56.50625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "SUyzaIbw5viP",
        "outputId": "3475cdb9-f197-4288-8a94-e999d9ed36fb"
      },
      "source": [
        "plot_metric(history_exp_decay, 'accuracy')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 398.50625 277.314375\" width=\"398.50625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 398.50625 277.314375 \nL 398.50625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 56.50625 239.758125 \nL 391.30625 239.758125 \nL 391.30625 22.318125 \nL 56.50625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"me422f0ed2e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"68.650052\" xlink:href=\"#me422f0ed2e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(65.468802 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.137655\" xlink:href=\"#me422f0ed2e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(123.775155 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"191.625258\" xlink:href=\"#me422f0ed2e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(185.262758 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"253.112862\" xlink:href=\"#me422f0ed2e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(246.750362 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"314.600465\" xlink:href=\"#me422f0ed2e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(308.237965 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"376.088068\" xlink:href=\"#me422f0ed2e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(366.544318 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Epochs -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(205.990625 268.034687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"306.201172\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mfed727e620\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#mfed727e620\" y=\"229.874458\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.106 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(20.878125 233.673676)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#mfed727e620\" y=\"205.036013\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.107 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(20.878125 208.835232)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#mfed727e620\" y=\"180.197569\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.108 -->\n      <g transform=\"translate(20.878125 183.996788)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#mfed727e620\" y=\"155.359124\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.109 -->\n      <defs>\n       <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n      </defs>\n      <g transform=\"translate(20.878125 159.158343)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-57\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#mfed727e620\" y=\"130.52068\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.110 -->\n      <g transform=\"translate(20.878125 134.319899)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#mfed727e620\" y=\"105.682236\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.111 -->\n      <g transform=\"translate(20.878125 109.481455)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#mfed727e620\" y=\"80.843791\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.112 -->\n      <g transform=\"translate(20.878125 84.64301)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#mfed727e620\" y=\"56.005347\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.113 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(20.878125 59.804566)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#mfed727e620\" y=\"31.166903\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.114 -->\n      <g transform=\"translate(20.878125 34.966121)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- accuracy -->\n     <defs>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n     </defs>\n     <g transform=\"translate(14.798437 153.5975)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#pfa554ac0a8)\" d=\"M 71.724432 35.306712 \nL 74.798812 32.201761 \nL 77.873192 32.201761 \nL 80.947572 32.201761 \nL 84.021952 32.201761 \nL 87.096333 32.201761 \nL 90.170713 32.201761 \nL 93.245093 32.201761 \nL 96.319473 32.201761 \nL 99.393853 32.201761 \nL 102.468233 32.201761 \nL 105.542614 32.201761 \nL 108.616994 32.201761 \nL 111.691374 32.201761 \nL 114.765754 32.201761 \nL 117.840134 32.201761 \nL 120.914514 32.201761 \nL 123.988895 32.201761 \nL 127.063275 32.201761 \nL 130.137655 32.201761 \nL 133.212035 32.201761 \nL 136.286415 32.201761 \nL 139.360795 32.201761 \nL 142.435176 32.201761 \nL 145.509556 32.201761 \nL 148.583936 32.201761 \nL 151.658316 32.201761 \nL 154.732696 32.201761 \nL 157.807076 32.201761 \nL 160.881457 32.201761 \nL 163.955837 32.201761 \nL 167.030217 32.201761 \nL 170.104597 32.201761 \nL 173.178977 32.201761 \nL 176.253357 32.201761 \nL 179.327738 32.201761 \nL 182.402118 32.201761 \nL 185.476498 32.201761 \nL 188.550878 32.201761 \nL 191.625258 32.201761 \nL 194.699638 32.201761 \nL 197.774019 32.201761 \nL 200.848399 32.201761 \nL 203.922779 32.201761 \nL 206.997159 32.201761 \nL 210.071539 32.201761 \nL 213.145919 32.201761 \nL 216.2203 32.201761 \nL 219.29468 32.201761 \nL 222.36906 32.201761 \nL 225.44344 32.201761 \nL 228.51782 32.201761 \nL 231.5922 32.201761 \nL 234.666581 32.201761 \nL 237.740961 32.201761 \nL 240.815341 32.201761 \nL 243.889721 32.201761 \nL 246.964101 32.201761 \nL 250.038481 32.201761 \nL 253.112862 32.201761 \nL 256.187242 32.201761 \nL 259.261622 32.201761 \nL 262.336002 32.201761 \nL 265.410382 32.201761 \nL 268.484762 32.201761 \nL 271.559143 32.201761 \nL 274.633523 32.201761 \nL 277.707903 32.201761 \nL 280.782283 32.201761 \nL 283.856663 32.201761 \nL 286.931043 32.201761 \nL 290.005424 32.201761 \nL 293.079804 32.201761 \nL 296.154184 32.201761 \nL 299.228564 32.201761 \nL 302.302944 32.201761 \nL 305.377324 32.201761 \nL 308.451705 32.201761 \nL 311.526085 32.201761 \nL 314.600465 32.201761 \nL 317.674845 32.201761 \nL 320.749225 32.201761 \nL 323.823605 32.201761 \nL 326.897986 32.201761 \nL 329.972366 32.201761 \nL 333.046746 32.201761 \nL 336.121126 32.201761 \nL 339.195506 32.201761 \nL 342.269886 32.201761 \nL 345.344267 32.201761 \nL 348.418647 32.201761 \nL 351.493027 32.201761 \nL 354.567407 32.201761 \nL 357.641787 32.201761 \nL 360.716167 32.201761 \nL 363.790548 32.201761 \nL 366.864928 32.201761 \nL 369.939308 32.201761 \nL 373.013688 32.201761 \nL 376.088068 32.201761 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#pfa554ac0a8)\" d=\"M 71.724432 229.874489 \nL 74.798812 229.874489 \nL 77.873192 229.874489 \nL 80.947572 229.874489 \nL 84.021952 229.874489 \nL 87.096333 229.874489 \nL 90.170713 229.874489 \nL 93.245093 229.874489 \nL 96.319473 229.874489 \nL 99.393853 229.874489 \nL 102.468233 229.874489 \nL 105.542614 229.874489 \nL 108.616994 229.874489 \nL 111.691374 229.874489 \nL 114.765754 229.874489 \nL 117.840134 229.874489 \nL 120.914514 229.874489 \nL 123.988895 229.874489 \nL 127.063275 229.874489 \nL 130.137655 229.874489 \nL 133.212035 229.874489 \nL 136.286415 229.874489 \nL 139.360795 229.874489 \nL 142.435176 229.874489 \nL 145.509556 229.874489 \nL 148.583936 229.874489 \nL 151.658316 229.874489 \nL 154.732696 229.874489 \nL 157.807076 229.874489 \nL 160.881457 229.874489 \nL 163.955837 229.874489 \nL 167.030217 229.874489 \nL 170.104597 229.874489 \nL 173.178977 229.874489 \nL 176.253357 229.874489 \nL 179.327738 229.874489 \nL 182.402118 229.874489 \nL 185.476498 229.874489 \nL 188.550878 229.874489 \nL 191.625258 229.874489 \nL 194.699638 229.874489 \nL 197.774019 229.874489 \nL 200.848399 229.874489 \nL 203.922779 229.874489 \nL 206.997159 229.874489 \nL 210.071539 229.874489 \nL 213.145919 229.874489 \nL 216.2203 229.874489 \nL 219.29468 229.874489 \nL 222.36906 229.874489 \nL 225.44344 229.874489 \nL 228.51782 229.874489 \nL 231.5922 229.874489 \nL 234.666581 229.874489 \nL 237.740961 229.874489 \nL 240.815341 229.874489 \nL 243.889721 229.874489 \nL 246.964101 229.874489 \nL 250.038481 229.874489 \nL 253.112862 229.874489 \nL 256.187242 229.874489 \nL 259.261622 229.874489 \nL 262.336002 229.874489 \nL 265.410382 229.874489 \nL 268.484762 229.874489 \nL 271.559143 229.874489 \nL 274.633523 229.874489 \nL 277.707903 229.874489 \nL 280.782283 229.874489 \nL 283.856663 229.874489 \nL 286.931043 229.874489 \nL 290.005424 229.874489 \nL 293.079804 229.874489 \nL 296.154184 229.874489 \nL 299.228564 229.874489 \nL 302.302944 229.874489 \nL 305.377324 229.874489 \nL 308.451705 229.874489 \nL 311.526085 229.874489 \nL 314.600465 229.874489 \nL 317.674845 229.874489 \nL 320.749225 229.874489 \nL 323.823605 229.874489 \nL 326.897986 229.874489 \nL 329.972366 229.874489 \nL 333.046746 229.874489 \nL 336.121126 229.874489 \nL 339.195506 229.874489 \nL 342.269886 229.874489 \nL 345.344267 229.874489 \nL 348.418647 229.874489 \nL 351.493027 229.874489 \nL 354.567407 229.874489 \nL 357.641787 229.874489 \nL 360.716167 229.874489 \nL 363.790548 229.874489 \nL 366.864928 229.874489 \nL 369.939308 229.874489 \nL 373.013688 229.874489 \nL 376.088068 229.874489 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 56.50625 239.758125 \nL 56.50625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 391.30625 239.758125 \nL 391.30625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 56.50625 239.758125 \nL 391.30625 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 56.50625 22.318125 \nL 391.30625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_18\">\n    <!-- Training and validation accuracy -->\n    <defs>\n     <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n     <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n    </defs>\n    <g transform=\"translate(126.614375 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"87.447266\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"148.726562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"176.509766\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"239.888672\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"267.671875\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"331.050781\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"394.527344\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"426.314453\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"487.59375\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"550.972656\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"614.449219\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"646.236328\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"705.416016\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"766.695312\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"794.478516\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"822.261719\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"885.738281\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"947.017578\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"986.226562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1014.009766\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1075.191406\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1138.570312\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1170.357422\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1231.636719\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1286.617188\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1341.597656\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"1404.976562\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1446.089844\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1507.369141\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1562.349609\" xlink:href=\"#DejaVuSans-121\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 278.9125 147.494375 \nL 384.30625 147.494375 \nQ 386.30625 147.494375 386.30625 145.494375 \nL 386.30625 116.581875 \nQ 386.30625 114.581875 384.30625 114.581875 \nL 278.9125 114.581875 \nQ 276.9125 114.581875 276.9125 116.581875 \nL 276.9125 145.494375 \nQ 276.9125 147.494375 278.9125 147.494375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 280.9125 122.680312 \nL 300.9125 122.680312 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_19\">\n     <!-- train_accuracy -->\n     <defs>\n      <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n     </defs>\n     <g transform=\"translate(308.9125 126.180312)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"282.763672\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"344.042969\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"399.023438\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"454.003906\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"517.382812\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"558.496094\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"619.775391\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"674.755859\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 280.9125 137.636562 \nL 300.9125 137.636562 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_20\">\n     <!-- val_accuracy -->\n     <g transform=\"translate(308.9125 141.136562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"259.521484\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"314.501953\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"369.482422\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"432.861328\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"473.974609\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"535.253906\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"590.234375\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pfa554ac0a8\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"56.50625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9cLW_bJ6aEf"
      },
      "source": [
        "Step 5 - Use sigmoid, relu, and softmax as activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COeGQILX6kmM"
      },
      "source": [
        "# Defining function for creating model with sigmoid, relu and softmax as activation functions\n",
        "\n",
        "def create_model2(): \n",
        "    model = Sequential([Flatten(input_shape=(28, 28)),\n",
        "        Dense(300, activation='sigmoid'),\n",
        "        Dense(100, activation='relu'),\n",
        "        Dense(10, activation='softmax')])\n",
        "    return model"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spy_V70v6cpf",
        "outputId": "e46790a2-1358-4760-d80b-6eb64a571172"
      },
      "source": [
        "# Creating model with sigmoid, relu and softmax\n",
        "\n",
        "model2 = create_model2()\n",
        "model2.compile(optimizer='sgd', # Learning rate defaults to 0.01\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "model2.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXYqDRCN638e",
        "outputId": "325ab48a-feca-4908-a338-c2335e000bf3"
      },
      "source": [
        "# Training model\n",
        "\n",
        "initial_learning_rate = 0.01\n",
        "def lr_exp_decay(epoch, lr):\n",
        "    k = 0.1\n",
        "    return initial_learning_rate * math.exp(-k*epoch)\n",
        "# Fit the model to the training data\n",
        "history_exp_decay2 = model2.fit(\n",
        "    x_train, \n",
        "    Y_train, \n",
        "    epochs=100, \n",
        "    validation_split=0.2,\n",
        "    batch_size=64,\n",
        "    callbacks=[keras.callbacks.LearningRateScheduler(lr_exp_decay, verbose=1)],\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.01.\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='flatten_2_input'), name='flatten_2_input', description=\"created by layer 'flatten_2_input'\"), but it was called on an input with incompatible shape (64, 784).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='flatten_2_input'), name='flatten_2_input', description=\"created by layer 'flatten_2_input'\"), but it was called on an input with incompatible shape (64, 784).\n",
            "745/750 [============================>.] - ETA: 0s - loss: 0.7011 - accuracy: 0.8163WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='flatten_2_input'), name='flatten_2_input', description=\"created by layer 'flatten_2_input'\"), but it was called on an input with incompatible shape (None, 784).\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.6989 - accuracy: 0.8168 - val_loss: 0.3497 - val_accuracy: 0.9053\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.009048374180359595.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3266 - accuracy: 0.9071 - val_loss: 0.2777 - val_accuracy: 0.9221\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.008187307530779819.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.2653 - accuracy: 0.9226 - val_loss: 0.2442 - val_accuracy: 0.9316\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.007408182206817179.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.2317 - accuracy: 0.9340 - val_loss: 0.2237 - val_accuracy: 0.9355\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.006703200460356393.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.2094 - accuracy: 0.9396 - val_loss: 0.2093 - val_accuracy: 0.9395\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.006065306597126334.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1893 - accuracy: 0.9456 - val_loss: 0.1968 - val_accuracy: 0.9434\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.005488116360940264.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1753 - accuracy: 0.9498 - val_loss: 0.1868 - val_accuracy: 0.9459\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.004965853037914095.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1628 - accuracy: 0.9532 - val_loss: 0.1810 - val_accuracy: 0.9477\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.004493289641172216.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1531 - accuracy: 0.9564 - val_loss: 0.1717 - val_accuracy: 0.9498\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.004065696597405992.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1434 - accuracy: 0.9588 - val_loss: 0.1692 - val_accuracy: 0.9500\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0036787944117144234.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1351 - accuracy: 0.9615 - val_loss: 0.1670 - val_accuracy: 0.9506\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.003328710836980796.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1275 - accuracy: 0.9645 - val_loss: 0.1617 - val_accuracy: 0.9525\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0030119421191220205.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1188 - accuracy: 0.9673 - val_loss: 0.1612 - val_accuracy: 0.9534\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.002725317930340126.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1126 - accuracy: 0.9697 - val_loss: 0.1563 - val_accuracy: 0.9545\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0024659696394160645.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1064 - accuracy: 0.9716 - val_loss: 0.1547 - val_accuracy: 0.9534\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0022313016014842983.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1001 - accuracy: 0.9736 - val_loss: 0.1523 - val_accuracy: 0.9552\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.002018965179946554.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0939 - accuracy: 0.9760 - val_loss: 0.1508 - val_accuracy: 0.9563\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.001826835240527346.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0891 - accuracy: 0.9775 - val_loss: 0.1493 - val_accuracy: 0.9556\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0016529888822158654.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0848 - accuracy: 0.9786 - val_loss: 0.1493 - val_accuracy: 0.9562\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0014956861922263505.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0813 - accuracy: 0.9800 - val_loss: 0.1491 - val_accuracy: 0.9559\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0013533528323661271.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0778 - accuracy: 0.9815 - val_loss: 0.1489 - val_accuracy: 0.9563\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.001224564282529819.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0751 - accuracy: 0.9824 - val_loss: 0.1484 - val_accuracy: 0.9571\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0011080315836233387.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0728 - accuracy: 0.9833 - val_loss: 0.1476 - val_accuracy: 0.9576\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.001002588437228037.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0710 - accuracy: 0.9837 - val_loss: 0.1478 - val_accuracy: 0.9572\n",
            "Epoch 25/100\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0009071795328941247.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0697 - accuracy: 0.9844 - val_loss: 0.1474 - val_accuracy: 0.9576\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0008208499862389881.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0684 - accuracy: 0.9847 - val_loss: 0.1473 - val_accuracy: 0.9574\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0007427357821433388.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0674 - accuracy: 0.9847 - val_loss: 0.1474 - val_accuracy: 0.9572\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0006720551273974976.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0666 - accuracy: 0.9851 - val_loss: 0.1475 - val_accuracy: 0.9572\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0006081006262521795.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0659 - accuracy: 0.9852 - val_loss: 0.1473 - val_accuracy: 0.9571\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0005502322005640721.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0653 - accuracy: 0.9855 - val_loss: 0.1470 - val_accuracy: 0.9572\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0004978706836786395.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0648 - accuracy: 0.9856 - val_loss: 0.1470 - val_accuracy: 0.9567\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.000450492023935578.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0644 - accuracy: 0.9857 - val_loss: 0.1470 - val_accuracy: 0.9569\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0004076220397836621.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0640 - accuracy: 0.9858 - val_loss: 0.1470 - val_accuracy: 0.9566\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.00036883167401239995.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0637 - accuracy: 0.9858 - val_loss: 0.1469 - val_accuracy: 0.9566\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0003337326996032607.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0634 - accuracy: 0.9859 - val_loss: 0.1469 - val_accuracy: 0.9567\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.00030197383422318503.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0632 - accuracy: 0.9859 - val_loss: 0.1469 - val_accuracy: 0.9570\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002732372244729256.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0630 - accuracy: 0.9860 - val_loss: 0.1468 - val_accuracy: 0.9567\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.00024723526470339386.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0628 - accuracy: 0.9860 - val_loss: 0.1468 - val_accuracy: 0.9569\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.00022370771856165591.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0626 - accuracy: 0.9861 - val_loss: 0.1468 - val_accuracy: 0.9571\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002024191144580438.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0624 - accuracy: 0.9861 - val_loss: 0.1468 - val_accuracy: 0.9569\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 0.00018315638888734178.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0623 - accuracy: 0.9862 - val_loss: 0.1467 - val_accuracy: 0.9568\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 0.00016572675401761236.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0622 - accuracy: 0.9861 - val_loss: 0.1467 - val_accuracy: 0.9569\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 0.00014995576820477704.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0621 - accuracy: 0.9862 - val_loss: 0.1467 - val_accuracy: 0.9569\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 0.00013568559012200934.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0620 - accuracy: 0.9862 - val_loss: 0.1467 - val_accuracy: 0.9568\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 0.00012277339903068437.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0619 - accuracy: 0.9862 - val_loss: 0.1467 - val_accuracy: 0.9568\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 0.00011108996538242307.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0618 - accuracy: 0.9862 - val_loss: 0.1467 - val_accuracy: 0.9569\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 0.00010051835744633576.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0617 - accuracy: 0.9862 - val_loss: 0.1467 - val_accuracy: 0.9569\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 9.095277101695816e-05.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0616 - accuracy: 0.9862 - val_loss: 0.1467 - val_accuracy: 0.9567\n",
            "Epoch 49/100\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 8.229747049020023e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0616 - accuracy: 0.9862 - val_loss: 0.1466 - val_accuracy: 0.9569\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 7.446583070924338e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0615 - accuracy: 0.9862 - val_loss: 0.1466 - val_accuracy: 0.9568\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 00051: LearningRateScheduler setting learning rate to 6.737946999085467e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0615 - accuracy: 0.9862 - val_loss: 0.1466 - val_accuracy: 0.9569\n",
            "Epoch 52/100\n",
            "\n",
            "Epoch 00052: LearningRateScheduler setting learning rate to 6.096746565515633e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0614 - accuracy: 0.9862 - val_loss: 0.1466 - val_accuracy: 0.9569\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 00053: LearningRateScheduler setting learning rate to 5.5165644207607714e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0614 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9568\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 00054: LearningRateScheduler setting learning rate to 4.991593906910213e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0614 - accuracy: 0.9862 - val_loss: 0.1466 - val_accuracy: 0.9568\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 00055: LearningRateScheduler setting learning rate to 4.516580942612666e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0613 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 00056: LearningRateScheduler setting learning rate to 4.086771438464067e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0613 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 57/100\n",
            "\n",
            "Epoch 00057: LearningRateScheduler setting learning rate to 3.697863716482929e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0613 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9566\n",
            "Epoch 58/100\n",
            "\n",
            "Epoch 00058: LearningRateScheduler setting learning rate to 3.3459654574712724e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0612 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9566\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 00059: LearningRateScheduler setting learning rate to 3.027554745375813e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0612 - accuracy: 0.9862 - val_loss: 0.1466 - val_accuracy: 0.9566\n",
            "Epoch 60/100\n",
            "\n",
            "Epoch 00060: LearningRateScheduler setting learning rate to 2.7394448187683685e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0612 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9566\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 00061: LearningRateScheduler setting learning rate to 2.4787521766663585e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0612 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9566\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 00062: LearningRateScheduler setting learning rate to 2.2428677194858013e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0612 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 00063: LearningRateScheduler setting learning rate to 2.029430636295734e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0612 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 00064: LearningRateScheduler setting learning rate to 1.8363047770289057e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0611 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 00065: LearningRateScheduler setting learning rate to 1.661557273173934e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0611 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 66/100\n",
            "\n",
            "Epoch 00066: LearningRateScheduler setting learning rate to 1.5034391929775723e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0611 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 00067: LearningRateScheduler setting learning rate to 1.3603680375478928e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0611 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 68/100\n",
            "\n",
            "Epoch 00068: LearningRateScheduler setting learning rate to 1.230911902673481e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0611 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 00069: LearningRateScheduler setting learning rate to 1.1137751478448024e-05.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0611 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 00070: LearningRateScheduler setting learning rate to 1.0077854290485104e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0611 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 00071: LearningRateScheduler setting learning rate to 9.118819655545163e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0611 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 00072: LearningRateScheduler setting learning rate to 8.25104923265904e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0611 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 00073: LearningRateScheduler setting learning rate to 7.465858083766792e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0611 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 00074: LearningRateScheduler setting learning rate to 6.755387751938438e-06.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0611 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 00075: LearningRateScheduler setting learning rate to 6.112527611295723e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0611 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 00076: LearningRateScheduler setting learning rate to 5.530843701478336e-06.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 00077: LearningRateScheduler setting learning rate to 5.004514334406104e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 00078: LearningRateScheduler setting learning rate to 4.528271828867969e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 00079: LearningRateScheduler setting learning rate to 4.097349789797864e-06.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 00080: LearningRateScheduler setting learning rate to 3.707435404590882e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 00081: LearningRateScheduler setting learning rate to 3.3546262790251185e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 00082: LearningRateScheduler setting learning rate to 3.035391380788668e-06.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 00083: LearningRateScheduler setting learning rate to 2.7465356997214204e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 00084: LearningRateScheduler setting learning rate to 2.4851682710795183e-06.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 00085: LearningRateScheduler setting learning rate to 2.248673241788482e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 00086: LearningRateScheduler setting learning rate to 2.0346836901064417e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 00087: LearningRateScheduler setting learning rate to 1.841057936675792e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 00088: LearningRateScheduler setting learning rate to 1.6658581098763324e-06.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 00089: LearningRateScheduler setting learning rate to 1.507330750954765e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 00090: LearningRateScheduler setting learning rate to 1.363889264820114e-06.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 00091: LearningRateScheduler setting learning rate to 1.2340980408667956e-06.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 00092: LearningRateScheduler setting learning rate to 1.1166580849011478e-06.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 00093: LearningRateScheduler setting learning rate to 1.0103940183709325e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 00094: LearningRateScheduler setting learning rate to 9.142423147817327e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 00095: LearningRateScheduler setting learning rate to 8.272406555663223e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 00096: LearningRateScheduler setting learning rate to 7.48518298877006e-07.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 00097: LearningRateScheduler setting learning rate to 6.772873649085378e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 00098: LearningRateScheduler setting learning rate to 6.128349505322203e-07.\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 00099: LearningRateScheduler setting learning rate to 5.545159943217694e-07.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 00100: LearningRateScheduler setting learning rate to 5.017468205617528e-07.\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0610 - accuracy: 0.9863 - val_loss: 0.1466 - val_accuracy: 0.9567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "1TXn3sCp8hpq",
        "outputId": "060491fe-60de-4a15-fa22-8ed39ea6ed9e"
      },
      "source": [
        "plot_lr(history_exp_decay2)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 398.50625 277.314375\" width=\"398.50625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 398.50625 277.314375 \nL 398.50625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 56.50625 239.758125 \nL 391.30625 239.758125 \nL 391.30625 22.318125 \nL 56.50625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"me2f74409ab\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"68.650052\" xlink:href=\"#me2f74409ab\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(65.468802 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.137655\" xlink:href=\"#me2f74409ab\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(123.775155 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"191.625258\" xlink:href=\"#me2f74409ab\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(185.262758 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"253.112862\" xlink:href=\"#me2f74409ab\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(246.750362 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"314.600465\" xlink:href=\"#me2f74409ab\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(308.237965 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"376.088068\" xlink:href=\"#me2f74409ab\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(366.544318 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Epochs -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(205.990625 268.034687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"306.201172\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m102bd66408\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m102bd66408\" y=\"229.884407\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.000 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(20.878125 233.683626)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m102bd66408\" y=\"190.347877\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.002 -->\n      <g transform=\"translate(20.878125 194.147096)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m102bd66408\" y=\"150.811347\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.004 -->\n      <g transform=\"translate(20.878125 154.610566)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m102bd66408\" y=\"111.274817\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.006 -->\n      <g transform=\"translate(20.878125 115.074036)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m102bd66408\" y=\"71.738287\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.008 -->\n      <g transform=\"translate(20.878125 75.537506)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m102bd66408\" y=\"32.201757\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.010 -->\n      <g transform=\"translate(20.878125 36.000976)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- Learning rate -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n     </defs>\n     <g transform=\"translate(14.798437 164.49125)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"115.486328\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"176.765625\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"216.128906\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"279.507812\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"307.291016\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"370.669922\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"434.146484\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"465.933594\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"507.046875\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"568.326172\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"607.535156\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#peb9325d74a)\" d=\"M 71.724432 32.201761 \nL 74.798812 51.013745 \nL 77.873192 68.035533 \nL 80.947572 83.437499 \nL 84.021952 97.373767 \nL 87.096333 109.983817 \nL 90.170713 121.39387 \nL 93.245093 131.71811 \nL 96.319473 141.059867 \nL 99.393853 149.512639 \nL 102.468233 157.161023 \nL 105.542614 164.08157 \nL 108.616994 170.343537 \nL 111.691374 176.009602 \nL 114.765754 181.136466 \nL 117.840134 185.775448 \nL 120.914514 189.97297 \nL 123.988895 193.771045 \nL 127.063275 197.207686 \nL 130.137655 200.317285 \nL 133.212035 203.130971 \nL 136.286415 205.676896 \nL 139.360795 207.980545 \nL 142.435176 210.064974 \nL 145.509556 211.951042 \nL 148.583936 213.657628 \nL 151.658316 215.201809 \nL 154.732696 216.599044 \nL 157.807076 217.863313 \nL 160.881457 219.007272 \nL 163.955837 220.042368 \nL 167.030217 220.978962 \nL 170.104597 221.826427 \nL 173.178977 222.593245 \nL 176.253357 223.287091 \nL 179.327738 223.914908 \nL 182.402118 224.482981 \nL 185.476498 224.996995 \nL 188.550878 225.462094 \nL 191.625258 225.882932 \nL 194.699638 226.263723 \nL 197.774019 226.608277 \nL 200.848399 226.920042 \nL 203.922779 227.202139 \nL 206.997159 227.45739 \nL 210.071539 227.688351 \nL 213.145919 227.897334 \nL 216.2203 228.086429 \nL 219.29468 228.257529 \nL 222.36906 228.412347 \nL 225.44344 228.552432 \nL 228.51782 228.679186 \nL 231.5922 228.793878 \nL 234.666581 228.897656 \nL 237.740961 228.991558 \nL 240.815341 229.076523 \nL 243.889721 229.153404 \nL 246.964101 229.222968 \nL 250.038481 229.285912 \nL 253.112862 229.342867 \nL 256.187242 229.394401 \nL 259.261622 229.441031 \nL 262.336002 229.483224 \nL 265.410382 229.521402 \nL 268.484762 229.555946 \nL 271.559143 229.587203 \nL 274.633523 229.615486 \nL 277.707903 229.641077 \nL 280.782283 229.664233 \nL 283.856663 229.685186 \nL 286.931043 229.704144 \nL 290.005424 229.721298 \nL 293.079804 229.73682 \nL 296.154184 229.750865 \nL 299.228564 229.763573 \nL 302.302944 229.775072 \nL 305.377324 229.785477 \nL 308.451705 229.794891 \nL 311.526085 229.80341 \nL 314.600465 229.811118 \nL 317.674845 229.818092 \nL 320.749225 229.824403 \nL 323.823605 229.830113 \nL 326.897986 229.83528 \nL 329.972366 229.839955 \nL 333.046746 229.844185 \nL 336.121126 229.848013 \nL 339.195506 229.851476 \nL 342.269886 229.85461 \nL 345.344267 229.857446 \nL 348.418647 229.860011 \nL 351.493027 229.862333 \nL 354.567407 229.864434 \nL 357.641787 229.866334 \nL 360.716167 229.868054 \nL 363.790548 229.86961 \nL 366.864928 229.871019 \nL 369.939308 229.872293 \nL 373.013688 229.873445 \nL 376.088068 229.874489 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 56.50625 239.758125 \nL 56.50625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 391.30625 239.758125 \nL 391.30625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 56.50625 239.758125 \nL 391.30625 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 56.50625 22.318125 \nL 391.30625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_15\">\n    <!-- Learning rate -->\n    <g transform=\"translate(183.7625 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"53.962891\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"115.486328\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"176.765625\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"216.128906\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"279.507812\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"307.291016\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"370.669922\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"434.146484\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"465.933594\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"507.046875\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"568.326172\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"607.535156\" xlink:href=\"#DejaVuSans-101\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"peb9325d74a\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"56.50625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "45zzykuI8nkC",
        "outputId": "bfa4f6a3-6ed0-4678-8a91-64282fbe2a16"
      },
      "source": [
        "plot_metric(history_exp_decay2, 'accuracy')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 398.50625 277.314375\" width=\"398.50625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 398.50625 277.314375 \nL 398.50625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 56.50625 239.758125 \nL 391.30625 239.758125 \nL 391.30625 22.318125 \nL 56.50625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m75ffc80469\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"68.650052\" xlink:href=\"#m75ffc80469\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(65.468802 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.137655\" xlink:href=\"#m75ffc80469\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(123.775155 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"191.625258\" xlink:href=\"#m75ffc80469\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(185.262758 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"253.112862\" xlink:href=\"#m75ffc80469\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(246.750362 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"314.600465\" xlink:href=\"#m75ffc80469\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(308.237965 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"376.088068\" xlink:href=\"#m75ffc80469\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(366.544318 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Epochs -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(205.990625 268.034687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"306.201172\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"md602931c72\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#md602931c72\" y=\"220.304179\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.825 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(20.878125 224.103397)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#md602931c72\" y=\"191.156073\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.850 -->\n      <g transform=\"translate(20.878125 194.955291)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#md602931c72\" y=\"162.007967\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.875 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(20.878125 165.807185)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#md602931c72\" y=\"132.859861\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.900 -->\n      <defs>\n       <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n      </defs>\n      <g transform=\"translate(20.878125 136.65908)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#md602931c72\" y=\"103.711755\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.925 -->\n      <g transform=\"translate(20.878125 107.510974)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#md602931c72\" y=\"74.563649\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.950 -->\n      <g transform=\"translate(20.878125 78.362868)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#md602931c72\" y=\"45.415543\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.975 -->\n      <g transform=\"translate(20.878125 49.214762)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- accuracy -->\n     <defs>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n     </defs>\n     <g transform=\"translate(14.798437 153.5975)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#pba14d373da)\" d=\"M 71.724432 229.874489 \nL 74.798812 124.528332 \nL 77.873192 106.529395 \nL 80.947572 93.242742 \nL 84.021952 86.708661 \nL 87.096333 79.737459 \nL 90.170713 74.830869 \nL 93.245093 70.822982 \nL 96.319473 67.057979 \nL 99.393853 64.28897 \nL 102.468233 61.179786 \nL 105.542614 57.609159 \nL 108.616994 54.427145 \nL 111.691374 51.53666 \nL 114.765754 49.374825 \nL 117.840134 47.043007 \nL 120.914514 44.249606 \nL 123.988895 42.500707 \nL 127.063275 41.189068 \nL 130.137655 39.634476 \nL 133.212035 37.788424 \nL 136.286415 36.792568 \nL 139.360795 35.699489 \nL 142.435176 35.213722 \nL 145.509556 34.485003 \nL 148.583936 34.120643 \nL 151.658316 34.047813 \nL 154.732696 33.5863 \nL 157.807076 33.537724 \nL 160.881457 33.221941 \nL 163.955837 33.100464 \nL 167.030217 32.906158 \nL 170.104597 32.857581 \nL 173.178977 32.881835 \nL 176.253357 32.711851 \nL 179.327738 32.687528 \nL 182.402118 32.566052 \nL 185.476498 32.590375 \nL 188.550878 32.468898 \nL 191.625258 32.468898 \nL 194.699638 32.396068 \nL 197.774019 32.444645 \nL 200.848399 32.396068 \nL 203.922779 32.396068 \nL 206.997159 32.396068 \nL 210.071539 32.371745 \nL 213.145919 32.298915 \nL 216.2203 32.298915 \nL 219.29468 32.323168 \nL 222.36906 32.298915 \nL 225.44344 32.323168 \nL 228.51782 32.323168 \nL 231.5922 32.274592 \nL 234.666581 32.298915 \nL 237.740961 32.274592 \nL 240.815341 32.250338 \nL 243.889721 32.250338 \nL 246.964101 32.274592 \nL 250.038481 32.298915 \nL 253.112862 32.274592 \nL 256.187242 32.250338 \nL 259.261622 32.250338 \nL 262.336002 32.274592 \nL 265.410382 32.226015 \nL 268.484762 32.250338 \nL 271.559143 32.226015 \nL 274.633523 32.201761 \nL 277.707903 32.226015 \nL 280.782283 32.201761 \nL 283.856663 32.201761 \nL 286.931043 32.201761 \nL 290.005424 32.201761 \nL 293.079804 32.201761 \nL 296.154184 32.201761 \nL 299.228564 32.201761 \nL 302.302944 32.201761 \nL 305.377324 32.201761 \nL 308.451705 32.201761 \nL 311.526085 32.201761 \nL 314.600465 32.201761 \nL 317.674845 32.201761 \nL 320.749225 32.201761 \nL 323.823605 32.201761 \nL 326.897986 32.201761 \nL 329.972366 32.201761 \nL 333.046746 32.201761 \nL 336.121126 32.201761 \nL 339.195506 32.201761 \nL 342.269886 32.201761 \nL 345.344267 32.201761 \nL 348.418647 32.201761 \nL 351.493027 32.201761 \nL 354.567407 32.201761 \nL 357.641787 32.201761 \nL 360.716167 32.201761 \nL 363.790548 32.201761 \nL 366.864928 32.201761 \nL 369.939308 32.201761 \nL 373.013688 32.201761 \nL 376.088068 32.201761 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pba14d373da)\" d=\"M 71.724432 126.64159 \nL 74.798812 107.112385 \nL 77.873192 96.036073 \nL 80.947572 91.46952 \nL 84.021952 86.805883 \nL 87.096333 82.23933 \nL 90.170713 79.324522 \nL 93.245093 77.284164 \nL 96.319473 74.757969 \nL 99.393853 74.563663 \nL 102.468233 73.88352 \nL 105.542614 71.648855 \nL 108.616994 70.580099 \nL 111.691374 69.316967 \nL 114.765754 70.580099 \nL 117.840134 68.539741 \nL 120.914514 67.179455 \nL 123.988895 68.053904 \nL 127.063275 67.373762 \nL 130.137655 67.665291 \nL 133.212035 67.179455 \nL 136.286415 66.305006 \nL 139.360795 65.722086 \nL 142.435176 66.207853 \nL 145.509556 65.722086 \nL 148.583936 65.916393 \nL 151.658316 66.207853 \nL 154.732696 66.110699 \nL 157.807076 66.305006 \nL 160.881457 66.207853 \nL 163.955837 66.790842 \nL 167.030217 66.499312 \nL 170.104597 66.887995 \nL 173.178977 66.887995 \nL 176.253357 66.790842 \nL 179.327738 66.402159 \nL 182.402118 66.693689 \nL 185.476498 66.499312 \nL 188.550878 66.305006 \nL 191.625258 66.499312 \nL 194.699638 66.596466 \nL 197.774019 66.499312 \nL 200.848399 66.499312 \nL 203.922779 66.596466 \nL 206.997159 66.596466 \nL 210.071539 66.499312 \nL 213.145919 66.499312 \nL 216.2203 66.693689 \nL 219.29468 66.499312 \nL 222.36906 66.596466 \nL 225.44344 66.499312 \nL 228.51782 66.499312 \nL 231.5922 66.596466 \nL 234.666581 66.596466 \nL 237.740961 66.693689 \nL 240.815341 66.790842 \nL 243.889721 66.887995 \nL 246.964101 66.887995 \nL 250.038481 66.887995 \nL 253.112862 66.887995 \nL 256.187242 66.887995 \nL 259.261622 66.790842 \nL 262.336002 66.790842 \nL 265.410382 66.790842 \nL 268.484762 66.790842 \nL 271.559143 66.790842 \nL 274.633523 66.790842 \nL 277.707903 66.790842 \nL 280.782283 66.790842 \nL 283.856663 66.790842 \nL 286.931043 66.790842 \nL 290.005424 66.790842 \nL 293.079804 66.790842 \nL 296.154184 66.790842 \nL 299.228564 66.790842 \nL 302.302944 66.790842 \nL 305.377324 66.790842 \nL 308.451705 66.790842 \nL 311.526085 66.790842 \nL 314.600465 66.790842 \nL 317.674845 66.790842 \nL 320.749225 66.790842 \nL 323.823605 66.790842 \nL 326.897986 66.790842 \nL 329.972366 66.790842 \nL 333.046746 66.790842 \nL 336.121126 66.790842 \nL 339.195506 66.790842 \nL 342.269886 66.790842 \nL 345.344267 66.790842 \nL 348.418647 66.790842 \nL 351.493027 66.790842 \nL 354.567407 66.790842 \nL 357.641787 66.790842 \nL 360.716167 66.790842 \nL 363.790548 66.790842 \nL 366.864928 66.790842 \nL 369.939308 66.790842 \nL 373.013688 66.790842 \nL 376.088068 66.790842 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 56.50625 239.758125 \nL 56.50625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 391.30625 239.758125 \nL 391.30625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 56.50625 239.758125 \nL 391.30625 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 56.50625 22.318125 \nL 391.30625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_16\">\n    <!-- Training and validation accuracy -->\n    <defs>\n     <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n     <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n    </defs>\n    <g transform=\"translate(126.614375 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"87.447266\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"148.726562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"176.509766\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"239.888672\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"267.671875\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"331.050781\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"394.527344\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"426.314453\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"487.59375\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"550.972656\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"614.449219\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"646.236328\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"705.416016\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"766.695312\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"794.478516\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"822.261719\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"885.738281\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"947.017578\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"986.226562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1014.009766\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1075.191406\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1138.570312\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1170.357422\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1231.636719\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1286.617188\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1341.597656\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"1404.976562\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1446.089844\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1507.369141\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1562.349609\" xlink:href=\"#DejaVuSans-121\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 278.9125 234.758125 \nL 384.30625 234.758125 \nQ 386.30625 234.758125 386.30625 232.758125 \nL 386.30625 203.845625 \nQ 386.30625 201.845625 384.30625 201.845625 \nL 278.9125 201.845625 \nQ 276.9125 201.845625 276.9125 203.845625 \nL 276.9125 232.758125 \nQ 276.9125 234.758125 278.9125 234.758125 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 280.9125 209.944062 \nL 300.9125 209.944062 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_17\"/>\n    <g id=\"text_17\">\n     <!-- train_accuracy -->\n     <defs>\n      <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n     </defs>\n     <g transform=\"translate(308.9125 213.444062)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"282.763672\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"344.042969\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"399.023438\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"454.003906\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"517.382812\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"558.496094\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"619.775391\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"674.755859\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 280.9125 224.900312 \nL 300.9125 224.900312 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_18\">\n     <!-- val_accuracy -->\n     <g transform=\"translate(308.9125 228.400312)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"259.521484\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"314.501953\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"369.482422\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"432.861328\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"473.974609\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"535.253906\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"590.234375\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pba14d373da\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"56.50625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNlIOSPI9CJz"
      },
      "source": [
        "Step 6 - Plot loss as a function of learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "-vF_gLHi8-cF",
        "outputId": "807bbd73-b8f8-47c3-86bc-6e03ded067be"
      },
      "source": [
        "plot_metric(history_exp_decay2, 'loss')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 385.78125 277.314375 \nL 385.78125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \nL 378.58125 22.318125 \nL 43.78125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m8b5fa45c2f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.925052\" xlink:href=\"#m8b5fa45c2f\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(52.743802 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"117.412655\" xlink:href=\"#m8b5fa45c2f\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(111.050155 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.900258\" xlink:href=\"#m8b5fa45c2f\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(172.537758 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"240.387862\" xlink:href=\"#m8b5fa45c2f\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(234.025362 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"301.875465\" xlink:href=\"#m8b5fa45c2f\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(295.512965 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.363068\" xlink:href=\"#m8b5fa45c2f\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(353.819318 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Epochs -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(193.265625 268.034687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"306.201172\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m0e854bea37\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m0e854bea37\" y=\"217.79457\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.1 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(20.878125 221.593789)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m0e854bea37\" y=\"186.806438\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 190.605657)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m0e854bea37\" y=\"155.818307\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.3 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(20.878125 159.617526)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m0e854bea37\" y=\"124.830176\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 128.629395)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m0e854bea37\" y=\"93.842044\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(20.878125 97.641263)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m0e854bea37\" y=\"62.853913\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.6 -->\n      <g transform=\"translate(20.878125 66.653132)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m0e854bea37\" y=\"31.865782\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.7 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(20.878125 35.665)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- loss -->\n     <defs>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     </defs>\n     <g transform=\"translate(14.798438 140.695937)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p0165e28c99)\" d=\"M 58.999432 32.201761 \nL 62.073812 147.586874 \nL 65.148192 166.577375 \nL 68.222572 176.97765 \nL 71.296952 183.881272 \nL 74.371333 190.134774 \nL 77.445713 194.453441 \nL 80.520093 198.328736 \nL 83.594473 201.339328 \nL 86.668853 204.332081 \nL 89.743233 206.922711 \nL 92.817614 209.286115 \nL 95.891994 211.961598 \nL 98.966374 213.879284 \nL 102.040754 215.820526 \nL 105.115134 217.760039 \nL 108.189514 219.673137 \nL 111.263895 221.186876 \nL 114.338275 222.511732 \nL 117.412655 223.589185 \nL 120.487035 224.683256 \nL 123.561415 225.514951 \nL 126.635795 226.224084 \nL 129.710176 226.765716 \nL 132.784556 227.187889 \nL 135.858936 227.572515 \nL 138.933316 227.885558 \nL 142.007696 228.153088 \nL 145.082076 228.354514 \nL 148.156457 228.543299 \nL 151.230837 228.693034 \nL 154.305217 228.828196 \nL 157.379597 228.941941 \nL 160.453977 229.042387 \nL 163.528357 229.128621 \nL 166.602738 229.20543 \nL 169.677118 229.270808 \nL 172.751498 229.331705 \nL 175.825878 229.383752 \nL 178.900258 229.432753 \nL 181.974638 229.476967 \nL 185.049019 229.516481 \nL 188.123399 229.551583 \nL 191.197779 229.582693 \nL 194.272159 229.611046 \nL 197.346539 229.636551 \nL 200.420919 229.659915 \nL 203.4953 229.680812 \nL 206.56968 229.699394 \nL 209.64406 229.71665 \nL 212.71844 229.731855 \nL 215.79282 229.74571 \nL 218.8672 229.758253 \nL 221.941581 229.769604 \nL 225.015961 229.779893 \nL 228.090341 229.789105 \nL 231.164721 229.797434 \nL 234.239101 229.804937 \nL 237.313481 229.811803 \nL 240.387862 229.817898 \nL 243.462242 229.823501 \nL 246.536622 229.828567 \nL 249.611002 229.833076 \nL 252.685382 229.837211 \nL 255.759762 229.840935 \nL 258.834143 229.844313 \nL 261.908523 229.847375 \nL 264.982903 229.850138 \nL 268.057283 229.852619 \nL 271.131663 229.854865 \nL 274.206043 229.85692 \nL 277.280424 229.858754 \nL 280.354804 229.860396 \nL 283.429184 229.861901 \nL 286.503564 229.863269 \nL 289.577944 229.864477 \nL 292.652324 229.865579 \nL 295.726705 229.866567 \nL 298.801085 229.867451 \nL 301.875465 229.86827 \nL 304.949845 229.868998 \nL 308.024225 229.869626 \nL 311.098605 229.87023 \nL 314.172986 229.87075 \nL 317.247366 229.871208 \nL 320.321746 229.871653 \nL 323.396126 229.872023 \nL 326.470506 229.872361 \nL 329.544886 229.872684 \nL 332.619267 229.872957 \nL 335.693647 229.873185 \nL 338.768027 229.873409 \nL 341.842407 229.8736 \nL 344.916787 229.873766 \nL 347.991167 229.873928 \nL 351.065548 229.874059 \nL 354.139928 229.874194 \nL 357.214308 229.874297 \nL 360.288688 229.874395 \nL 363.363068 229.874489 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p0165e28c99)\" d=\"M 58.999432 140.414536 \nL 62.073812 162.737422 \nL 65.148192 173.120677 \nL 68.222572 179.449765 \nL 71.296952 183.932356 \nL 74.371333 187.783197 \nL 77.445713 190.892646 \nL 80.520093 192.693142 \nL 83.594473 195.577986 \nL 86.668853 196.341256 \nL 89.743233 197.017474 \nL 92.817614 198.661143 \nL 95.891994 198.844826 \nL 98.966374 200.339541 \nL 102.040754 200.839321 \nL 105.115134 201.58666 \nL 108.189514 202.062479 \nL 111.263895 202.511443 \nL 114.338275 202.502965 \nL 117.412655 202.574972 \nL 120.487035 202.633472 \nL 123.561415 202.799923 \nL 126.635795 203.04031 \nL 129.710176 202.987595 \nL 132.784556 203.100103 \nL 135.858936 203.144755 \nL 138.933316 203.105192 \nL 142.007696 203.078396 \nL 145.082076 203.133239 \nL 148.156457 203.235241 \nL 151.230837 203.217925 \nL 154.305217 203.232859 \nL 157.379597 203.243493 \nL 160.453977 203.249394 \nL 163.528357 203.258334 \nL 166.602738 203.274888 \nL 169.677118 203.288833 \nL 172.751498 203.285301 \nL 175.825878 203.298599 \nL 178.900258 203.288958 \nL 181.974638 203.315841 \nL 185.049019 203.315333 \nL 188.123399 203.317845 \nL 191.197779 203.332109 \nL 194.272159 203.330622 \nL 197.346539 203.334099 \nL 200.420919 203.335475 \nL 203.4953 203.336976 \nL 206.56968 203.339049 \nL 209.64406 203.339839 \nL 212.71844 203.341474 \nL 215.79282 203.343007 \nL 218.8672 203.345463 \nL 221.941581 203.346853 \nL 225.015961 203.349347 \nL 228.090341 203.35064 \nL 231.164721 203.352145 \nL 234.239101 203.353401 \nL 237.313481 203.354084 \nL 240.387862 203.354957 \nL 243.462242 203.356139 \nL 246.536622 203.357377 \nL 249.611002 203.358018 \nL 252.685382 203.359104 \nL 255.759762 203.359676 \nL 258.834143 203.360309 \nL 261.908523 203.361071 \nL 264.982903 203.361514 \nL 268.057283 203.362013 \nL 271.131663 203.362419 \nL 274.206043 203.362816 \nL 277.280424 203.363264 \nL 280.354804 203.363587 \nL 283.429184 203.363878 \nL 286.503564 203.364183 \nL 289.577944 203.364451 \nL 292.652324 203.364686 \nL 295.726705 203.364903 \nL 298.801085 203.36507 \nL 301.875465 203.365263 \nL 304.949845 203.365425 \nL 308.024225 203.365582 \nL 311.098605 203.365744 \nL 314.172986 203.365878 \nL 317.247366 203.366007 \nL 320.321746 203.366145 \nL 323.396126 203.366238 \nL 326.470506 203.366362 \nL 329.544886 203.366473 \nL 332.619267 203.366561 \nL 335.693647 203.366653 \nL 338.768027 203.366718 \nL 341.842407 203.36682 \nL 344.916787 203.366861 \nL 347.991167 203.366963 \nL 351.065548 203.367004 \nL 354.139928 203.367074 \nL 357.214308 203.36712 \nL 360.288688 203.367161 \nL 363.363068 203.367221 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 239.758125 \nL 43.78125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 378.58125 239.758125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 22.318125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_16\">\n    <!-- Training and validation loss -->\n    <defs>\n     <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n     <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n    </defs>\n    <g transform=\"translate(129.37125 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"87.447266\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"148.726562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"176.509766\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"239.888672\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"267.671875\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"331.050781\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"394.527344\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"426.314453\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"487.59375\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"550.972656\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"614.449219\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"646.236328\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"705.416016\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"766.695312\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"794.478516\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"822.261719\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"885.738281\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"947.017578\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"986.226562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1014.009766\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1075.191406\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1138.570312\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1170.357422\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"1198.140625\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1259.322266\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"1311.421875\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 291.990625 60.230625 \nL 371.58125 60.230625 \nQ 373.58125 60.230625 373.58125 58.230625 \nL 373.58125 29.318125 \nQ 373.58125 27.318125 371.58125 27.318125 \nL 291.990625 27.318125 \nQ 289.990625 27.318125 289.990625 29.318125 \nL 289.990625 58.230625 \nQ 289.990625 60.230625 291.990625 60.230625 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 293.990625 35.416562 \nL 313.990625 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_17\"/>\n    <g id=\"text_17\">\n     <!-- train_loss -->\n     <defs>\n      <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n     </defs>\n     <g transform=\"translate(321.990625 38.916562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"282.763672\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"310.546875\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"371.728516\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"423.828125\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 293.990625 50.372812 \nL 313.990625 50.372812 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_18\">\n     <!-- val_loss -->\n     <g transform=\"translate(321.990625 53.872812)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p0165e28c99\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "jzgll2_tKMsr",
        "outputId": "ffe65ae2-9cab-42c0-9257-51957a3a8387"
      },
      "source": [
        "plot_metric2(history_exp_decay2, 'loss')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.5925pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.5925\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.5925 \nL 385.78125 277.5925 \nL 385.78125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \nL 378.58125 22.318125 \nL 43.78125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m29371d2c99\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.98416\" xlink:href=\"#m29371d2c99\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.000 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(44.670097 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"119.859943\" xlink:href=\"#m29371d2c99\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.002 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(105.54588 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"180.735726\" xlink:href=\"#m29371d2c99\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.004 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(166.421663 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.611509\" xlink:href=\"#m29371d2c99\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.006 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(227.297446 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"302.487292\" xlink:href=\"#m29371d2c99\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.008 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(288.173229 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.363075\" xlink:href=\"#m29371d2c99\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0.010 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(349.049012 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Learning_rate -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n     </defs>\n     <g transform=\"translate(176.817187 268.034688)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"115.486328\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"176.765625\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"216.128906\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"279.507812\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"307.291016\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"370.669922\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"434.146484\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"484.146484\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"525.259766\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"586.539062\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"625.748047\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"ma10b110551\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma10b110551\" y=\"217.79457\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.1 -->\n      <g transform=\"translate(20.878125 221.593789)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma10b110551\" y=\"186.806438\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 190.605657)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma10b110551\" y=\"155.818307\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.3 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(20.878125 159.617526)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma10b110551\" y=\"124.830176\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 128.629395)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma10b110551\" y=\"93.842044\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(20.878125 97.641263)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma10b110551\" y=\"62.853913\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.6 -->\n      <g transform=\"translate(20.878125 66.653132)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma10b110551\" y=\"31.865782\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.7 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(20.878125 35.665)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- loss -->\n     <defs>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(14.798438 140.695938)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p02961cdce4)\" d=\"M 363.363068 32.201761 \nL 334.397597 147.586874 \nL 308.188552 166.577375 \nL 284.473604 176.97765 \nL 263.015443 183.881272 \nL 243.599308 190.134774 \nL 226.030848 194.453441 \nL 210.134253 198.328736 \nL 195.750422 201.339328 \nL 182.735392 204.332081 \nL 170.958907 206.922711 \nL 160.303098 209.286115 \nL 150.661328 211.961598 \nL 141.937089 213.879284 \nL 134.043076 215.820526 \nL 126.900273 217.760039 \nL 120.4372 219.673137 \nL 114.589172 221.186876 \nL 109.297655 222.511732 \nL 104.509695 223.589185 \nL 100.177365 224.683256 \nL 96.257314 225.514951 \nL 92.710305 226.224084 \nL 89.500837 226.765716 \nL 86.596792 227.187889 \nL 83.969102 227.572515 \nL 81.591472 227.885558 \nL 79.4401 228.153088 \nL 77.49346 228.354514 \nL 75.732067 228.543299 \nL 74.138293 228.693034 \nL 72.696187 228.828196 \nL 71.391315 228.941941 \nL 70.210618 229.042387 \nL 69.142279 229.128621 \nL 68.175607 229.20543 \nL 67.300925 229.270808 \nL 66.50948 229.331705 \nL 65.793351 229.383752 \nL 65.145371 229.432753 \nL 64.559054 229.476967 \nL 64.028533 229.516481 \nL 63.548497 229.551583 \nL 63.114143 229.582693 \nL 62.721123 229.611046 \nL 62.365504 229.636551 \nL 62.043727 229.659915 \nL 61.75257 229.680812 \nL 61.489121 229.699394 \nL 61.250743 229.71665 \nL 61.035049 229.731855 \nL 60.839881 229.74571 \nL 60.663286 229.758253 \nL 60.503496 229.769604 \nL 60.358912 229.779893 \nL 60.228087 229.789105 \nL 60.109711 229.797434 \nL 60.002601 229.804937 \nL 59.905684 229.811803 \nL 59.817989 229.817898 \nL 59.73864 229.823501 \nL 59.666841 229.828567 \nL 59.601876 229.833076 \nL 59.543092 229.837211 \nL 59.489903 229.840935 \nL 59.441775 229.844313 \nL 59.398227 229.847375 \nL 59.358823 229.850138 \nL 59.323169 229.852619 \nL 59.290908 229.854865 \nL 59.261717 229.85692 \nL 59.235304 229.858754 \nL 59.211405 229.860396 \nL 59.189779 229.861901 \nL 59.170212 229.863269 \nL 59.152507 229.864477 \nL 59.136487 229.865579 \nL 59.121991 229.866567 \nL 59.108874 229.867451 \nL 59.097006 229.86827 \nL 59.086267 229.868998 \nL 59.076551 229.869626 \nL 59.067758 229.87023 \nL 59.059803 229.87075 \nL 59.052605 229.871208 \nL 59.046091 229.871653 \nL 59.040198 229.872023 \nL 59.034865 229.872361 \nL 59.03004 229.872684 \nL 59.025674 229.872957 \nL 59.021723 229.873185 \nL 59.018148 229.873409 \nL 59.014914 229.8736 \nL 59.011987 229.873766 \nL 59.009339 229.873928 \nL 59.006943 229.874059 \nL 59.004775 229.874194 \nL 59.002813 229.874297 \nL 59.001038 229.874395 \nL 58.999432 229.874489 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p02961cdce4)\" d=\"M 363.363068 140.414536 \nL 334.397597 162.737422 \nL 308.188552 173.120677 \nL 284.473604 179.449765 \nL 263.015443 183.932356 \nL 243.599308 187.783197 \nL 226.030848 190.892646 \nL 210.134253 192.693142 \nL 195.750422 195.577986 \nL 182.735392 196.341256 \nL 170.958907 197.017474 \nL 160.303098 198.661143 \nL 150.661328 198.844826 \nL 141.937089 200.339541 \nL 134.043076 200.839321 \nL 126.900273 201.58666 \nL 120.4372 202.062479 \nL 114.589172 202.511443 \nL 109.297655 202.502965 \nL 104.509695 202.574972 \nL 100.177365 202.633472 \nL 96.257314 202.799923 \nL 92.710305 203.04031 \nL 89.500837 202.987595 \nL 86.596792 203.100103 \nL 83.969102 203.144755 \nL 81.591472 203.105192 \nL 79.4401 203.078396 \nL 77.49346 203.133239 \nL 75.732067 203.235241 \nL 74.138293 203.217925 \nL 72.696187 203.232859 \nL 71.391315 203.243493 \nL 70.210618 203.249394 \nL 69.142279 203.258334 \nL 68.175607 203.274888 \nL 67.300925 203.288833 \nL 66.50948 203.285301 \nL 65.793351 203.298599 \nL 65.145371 203.288958 \nL 64.559054 203.315841 \nL 64.028533 203.315333 \nL 63.548497 203.317845 \nL 63.114143 203.332109 \nL 62.721123 203.330622 \nL 62.365504 203.334099 \nL 62.043727 203.335475 \nL 61.75257 203.336976 \nL 61.489121 203.339049 \nL 61.250743 203.339839 \nL 61.035049 203.341474 \nL 60.839881 203.343007 \nL 60.663286 203.345463 \nL 60.503496 203.346853 \nL 60.358912 203.349347 \nL 60.228087 203.35064 \nL 60.109711 203.352145 \nL 60.002601 203.353401 \nL 59.905684 203.354084 \nL 59.817989 203.354957 \nL 59.73864 203.356139 \nL 59.666841 203.357377 \nL 59.601876 203.358018 \nL 59.543092 203.359104 \nL 59.489903 203.359676 \nL 59.441775 203.360309 \nL 59.398227 203.361071 \nL 59.358823 203.361514 \nL 59.323169 203.362013 \nL 59.290908 203.362419 \nL 59.261717 203.362816 \nL 59.235304 203.363264 \nL 59.211405 203.363587 \nL 59.189779 203.363878 \nL 59.170212 203.364183 \nL 59.152507 203.364451 \nL 59.136487 203.364686 \nL 59.121991 203.364903 \nL 59.108874 203.36507 \nL 59.097006 203.365263 \nL 59.086267 203.365425 \nL 59.076551 203.365582 \nL 59.067758 203.365744 \nL 59.059803 203.365878 \nL 59.052605 203.366007 \nL 59.046091 203.366145 \nL 59.040198 203.366238 \nL 59.034865 203.366362 \nL 59.03004 203.366473 \nL 59.025674 203.366561 \nL 59.021723 203.366653 \nL 59.018148 203.366718 \nL 59.014914 203.36682 \nL 59.011987 203.366861 \nL 59.009339 203.366963 \nL 59.006943 203.367004 \nL 59.004775 203.367074 \nL 59.002813 203.36712 \nL 59.001038 203.367161 \nL 58.999432 203.367221 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 239.758125 \nL 43.78125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 378.58125 239.758125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 22.318125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_16\">\n    <!-- Training and validation learning rate vs loss loss -->\n    <defs>\n     <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n    </defs>\n    <g transform=\"translate(66.811875 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"87.447266\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"148.726562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"176.509766\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"239.888672\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"267.671875\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"331.050781\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"394.527344\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"426.314453\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"487.59375\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"550.972656\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"614.449219\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"646.236328\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"705.416016\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"766.695312\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"794.478516\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"822.261719\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"885.738281\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"947.017578\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"986.226562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1014.009766\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1075.191406\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1138.570312\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1170.357422\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"1198.140625\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1259.664062\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1320.943359\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1360.306641\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1423.685547\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1451.46875\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1514.847656\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"1578.324219\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1610.111328\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1651.224609\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1712.503906\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1751.712891\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1813.236328\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1845.023438\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"1904.203125\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"1956.302734\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1988.089844\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"2015.873047\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"2077.054688\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2129.154297\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2181.253906\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2213.041016\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"2240.824219\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"2302.005859\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2354.105469\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 50.78125 60.230625 \nL 130.371875 60.230625 \nQ 132.371875 60.230625 132.371875 58.230625 \nL 132.371875 29.318125 \nQ 132.371875 27.318125 130.371875 27.318125 \nL 50.78125 27.318125 \nQ 48.78125 27.318125 48.78125 29.318125 \nL 48.78125 58.230625 \nQ 48.78125 60.230625 50.78125 60.230625 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 52.78125 35.416563 \nL 72.78125 35.416563 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_17\"/>\n    <g id=\"text_17\">\n     <!-- train_loss -->\n     <g transform=\"translate(80.78125 38.916563)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"282.763672\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"310.546875\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"371.728516\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"423.828125\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 52.78125 50.372813 \nL 72.78125 50.372813 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_18\">\n     <!-- val_loss -->\n     <g transform=\"translate(80.78125 53.872813)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p02961cdce4\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjAXqhU_4xUg"
      },
      "source": [
        "Step 7 - What is the value of lr when loss shoots up? Report answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyeCUighdUov"
      },
      "source": [
        "Loss shoots up when learning rate is **0.009** for training and validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOFlpeNK418l"
      },
      "source": [
        "Step 8 - compile losses, use various optimizers. Check the documentation on losses to learn more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Dg9yMIc49O5",
        "outputId": "fa0ff732-6e6b-4071-bbef-b08b01c3c189"
      },
      "source": [
        "model_adam = create_model2()\n",
        "model_adam.compile(optimizer='adam', # Learning rate defaults to 0.01\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "model_adam.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_3 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoAh9D6jJVbX",
        "outputId": "088d7dde-fd14-49aa-a7bd-fff3470b5793"
      },
      "source": [
        "initial_learning_rate = 0.005\n",
        "def lr_exp_decay(epoch, lr):\n",
        "    k = 0.1\n",
        "    return initial_learning_rate * math.exp(-k*epoch)\n",
        "# Fit the model to the training data\n",
        "history_exp_decay_adam = model_adam.fit(\n",
        "    x_train, \n",
        "    Y_train, \n",
        "    epochs=100, \n",
        "    validation_split=0.2,\n",
        "    batch_size=64,\n",
        "    callbacks=[keras.callbacks.LearningRateScheduler(lr_exp_decay, verbose=1)],\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.005.\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='flatten_3_input'), name='flatten_3_input', description=\"created by layer 'flatten_3_input'\"), but it was called on an input with incompatible shape (64, 784).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='flatten_3_input'), name='flatten_3_input', description=\"created by layer 'flatten_3_input'\"), but it was called on an input with incompatible shape (64, 784).\n",
            "741/750 [============================>.] - ETA: 0s - loss: 0.6566 - accuracy: 0.7864WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='flatten_3_input'), name='flatten_3_input', description=\"created by layer 'flatten_3_input'\"), but it was called on an input with incompatible shape (None, 784).\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.6555 - accuracy: 0.7867 - val_loss: 0.5279 - val_accuracy: 0.8213\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0045241870901797975.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.5105 - accuracy: 0.8325 - val_loss: 0.4655 - val_accuracy: 0.8530\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0040936537653899095.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.4623 - accuracy: 0.8519 - val_loss: 0.4333 - val_accuracy: 0.8597\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0037040911034085895.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3901 - accuracy: 0.8755 - val_loss: 0.3657 - val_accuracy: 0.8846\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0033516002301781965.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3608 - accuracy: 0.8851 - val_loss: 0.3053 - val_accuracy: 0.9049\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.003032653298563167.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3250 - accuracy: 0.8974 - val_loss: 0.2940 - val_accuracy: 0.9065\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.002744058180470132.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3024 - accuracy: 0.9038 - val_loss: 0.2804 - val_accuracy: 0.9117\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0024829265189570474.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.2810 - accuracy: 0.9089 - val_loss: 0.2794 - val_accuracy: 0.9107\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.002246644820586108.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.2747 - accuracy: 0.9138 - val_loss: 0.2572 - val_accuracy: 0.9199\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.002032848298702996.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.2569 - accuracy: 0.9181 - val_loss: 0.2511 - val_accuracy: 0.9205\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0018393972058572117.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.2441 - accuracy: 0.9218 - val_loss: 0.2364 - val_accuracy: 0.9263\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.001664355418490398.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.2333 - accuracy: 0.9250 - val_loss: 0.2299 - val_accuracy: 0.9273\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0015059710595610102.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.2181 - accuracy: 0.9301 - val_loss: 0.2319 - val_accuracy: 0.9262\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.001362658965170063.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.2120 - accuracy: 0.9337 - val_loss: 0.2157 - val_accuracy: 0.9321\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0012329848197080323.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.2033 - accuracy: 0.9346 - val_loss: 0.2093 - val_accuracy: 0.9320\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0011156508007421492.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1925 - accuracy: 0.9380 - val_loss: 0.1990 - val_accuracy: 0.9370\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.001009482589973277.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1832 - accuracy: 0.9410 - val_loss: 0.1985 - val_accuracy: 0.9393\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.000913417620263673.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1764 - accuracy: 0.9440 - val_loss: 0.1947 - val_accuracy: 0.9380\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0008264944411079327.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1704 - accuracy: 0.9454 - val_loss: 0.1834 - val_accuracy: 0.9420\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0007478430961131752.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1671 - accuracy: 0.9465 - val_loss: 0.1794 - val_accuracy: 0.9433\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0006766764161830636.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1618 - accuracy: 0.9478 - val_loss: 0.1776 - val_accuracy: 0.9464\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0006122821412649095.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1547 - accuracy: 0.9499 - val_loss: 0.1778 - val_accuracy: 0.9452\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0005540157918116693.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1512 - accuracy: 0.9517 - val_loss: 0.1656 - val_accuracy: 0.9475\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0005012942186140185.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1480 - accuracy: 0.9531 - val_loss: 0.1693 - val_accuracy: 0.9472\n",
            "Epoch 25/100\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.00045358976644706235.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1412 - accuracy: 0.9551 - val_loss: 0.1705 - val_accuracy: 0.9469\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.00041042499311949403.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1391 - accuracy: 0.9541 - val_loss: 0.1631 - val_accuracy: 0.9496\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003713678910716694.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1342 - accuracy: 0.9572 - val_loss: 0.1614 - val_accuracy: 0.9489\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003360275636987488.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1315 - accuracy: 0.9579 - val_loss: 0.1619 - val_accuracy: 0.9503\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.00030405031312608977.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1299 - accuracy: 0.9582 - val_loss: 0.1624 - val_accuracy: 0.9497\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.00027511610028203604.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1277 - accuracy: 0.9588 - val_loss: 0.1615 - val_accuracy: 0.9503\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.00024893534183931975.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1259 - accuracy: 0.9601 - val_loss: 0.1625 - val_accuracy: 0.9520\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.000225246011967789.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1215 - accuracy: 0.9600 - val_loss: 0.1630 - val_accuracy: 0.9492\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.00020381101989183105.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1188 - accuracy: 0.9620 - val_loss: 0.1622 - val_accuracy: 0.9507\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.00018441583700619997.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1167 - accuracy: 0.9626 - val_loss: 0.1609 - val_accuracy: 0.9518\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00016686634980163034.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1146 - accuracy: 0.9632 - val_loss: 0.1595 - val_accuracy: 0.9518\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.00015098691711159251.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1124 - accuracy: 0.9644 - val_loss: 0.1613 - val_accuracy: 0.9511\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0001366186122364628.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1113 - accuracy: 0.9647 - val_loss: 0.1608 - val_accuracy: 0.9508\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.00012361763235169693.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1092 - accuracy: 0.9656 - val_loss: 0.1592 - val_accuracy: 0.9523\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.00011185385928082796.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1078 - accuracy: 0.9657 - val_loss: 0.1597 - val_accuracy: 0.9515\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0001012095572290219.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1057 - accuracy: 0.9657 - val_loss: 0.1571 - val_accuracy: 0.9528\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 9.157819444367089e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1046 - accuracy: 0.9670 - val_loss: 0.1568 - val_accuracy: 0.9526\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 8.286337700880618e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1031 - accuracy: 0.9674 - val_loss: 0.1551 - val_accuracy: 0.9529\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 7.497788410238852e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1016 - accuracy: 0.9677 - val_loss: 0.1553 - val_accuracy: 0.9525\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 6.784279506100467e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1005 - accuracy: 0.9677 - val_loss: 0.1575 - val_accuracy: 0.9525\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 6.138669951534219e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0988 - accuracy: 0.9684 - val_loss: 0.1572 - val_accuracy: 0.9528\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 5.5544982691211535e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0975 - accuracy: 0.9690 - val_loss: 0.1571 - val_accuracy: 0.9528\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 5.025917872316788e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0966 - accuracy: 0.9691 - val_loss: 0.1563 - val_accuracy: 0.9533\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 4.547638550847908e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0957 - accuracy: 0.9696 - val_loss: 0.1559 - val_accuracy: 0.9544\n",
            "Epoch 49/100\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 4.1148735245100114e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0945 - accuracy: 0.9697 - val_loss: 0.1559 - val_accuracy: 0.9529\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 3.723291535462169e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0936 - accuracy: 0.9699 - val_loss: 0.1563 - val_accuracy: 0.9532\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 00051: LearningRateScheduler setting learning rate to 3.3689734995427336e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0929 - accuracy: 0.9704 - val_loss: 0.1561 - val_accuracy: 0.9528\n",
            "Epoch 52/100\n",
            "\n",
            "Epoch 00052: LearningRateScheduler setting learning rate to 3.0483732827578164e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0921 - accuracy: 0.9709 - val_loss: 0.1558 - val_accuracy: 0.9536\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 00053: LearningRateScheduler setting learning rate to 2.7582822103803857e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0916 - accuracy: 0.9710 - val_loss: 0.1561 - val_accuracy: 0.9523\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 00054: LearningRateScheduler setting learning rate to 2.4957969534551064e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0910 - accuracy: 0.9711 - val_loss: 0.1556 - val_accuracy: 0.9526\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 00055: LearningRateScheduler setting learning rate to 2.258290471306333e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0905 - accuracy: 0.9713 - val_loss: 0.1552 - val_accuracy: 0.9538\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 00056: LearningRateScheduler setting learning rate to 2.0433857192320335e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0900 - accuracy: 0.9714 - val_loss: 0.1556 - val_accuracy: 0.9538\n",
            "Epoch 57/100\n",
            "\n",
            "Epoch 00057: LearningRateScheduler setting learning rate to 1.8489318582414646e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0896 - accuracy: 0.9716 - val_loss: 0.1555 - val_accuracy: 0.9535\n",
            "Epoch 58/100\n",
            "\n",
            "Epoch 00058: LearningRateScheduler setting learning rate to 1.6729827287356362e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0893 - accuracy: 0.9717 - val_loss: 0.1554 - val_accuracy: 0.9540\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 00059: LearningRateScheduler setting learning rate to 1.5137773726879064e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0891 - accuracy: 0.9718 - val_loss: 0.1553 - val_accuracy: 0.9533\n",
            "Epoch 60/100\n",
            "\n",
            "Epoch 00060: LearningRateScheduler setting learning rate to 1.3697224093841842e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0889 - accuracy: 0.9719 - val_loss: 0.1552 - val_accuracy: 0.9532\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 00061: LearningRateScheduler setting learning rate to 1.2393760883331792e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0886 - accuracy: 0.9719 - val_loss: 0.1548 - val_accuracy: 0.9539\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 00062: LearningRateScheduler setting learning rate to 1.1214338597429007e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0884 - accuracy: 0.9720 - val_loss: 0.1548 - val_accuracy: 0.9541\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 00063: LearningRateScheduler setting learning rate to 1.014715318147867e-05.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0883 - accuracy: 0.9721 - val_loss: 0.1549 - val_accuracy: 0.9538\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 00064: LearningRateScheduler setting learning rate to 9.181523885144529e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0882 - accuracy: 0.9721 - val_loss: 0.1549 - val_accuracy: 0.9536\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 00065: LearningRateScheduler setting learning rate to 8.30778636586967e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0881 - accuracy: 0.9721 - val_loss: 0.1548 - val_accuracy: 0.9539\n",
            "Epoch 66/100\n",
            "\n",
            "Epoch 00066: LearningRateScheduler setting learning rate to 7.517195964887862e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0879 - accuracy: 0.9722 - val_loss: 0.1548 - val_accuracy: 0.9543\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 00067: LearningRateScheduler setting learning rate to 6.801840187739464e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0879 - accuracy: 0.9722 - val_loss: 0.1548 - val_accuracy: 0.9542\n",
            "Epoch 68/100\n",
            "\n",
            "Epoch 00068: LearningRateScheduler setting learning rate to 6.154559513367405e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0878 - accuracy: 0.9722 - val_loss: 0.1547 - val_accuracy: 0.9538\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 00069: LearningRateScheduler setting learning rate to 5.568875739224012e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0877 - accuracy: 0.9722 - val_loss: 0.1547 - val_accuracy: 0.9541\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 00070: LearningRateScheduler setting learning rate to 5.038927145242552e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0876 - accuracy: 0.9724 - val_loss: 0.1547 - val_accuracy: 0.9538\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 00071: LearningRateScheduler setting learning rate to 4.559409827772581e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0875 - accuracy: 0.9722 - val_loss: 0.1547 - val_accuracy: 0.9539\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 00072: LearningRateScheduler setting learning rate to 4.12552461632952e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0875 - accuracy: 0.9725 - val_loss: 0.1548 - val_accuracy: 0.9538\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 00073: LearningRateScheduler setting learning rate to 3.732929041883396e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0874 - accuracy: 0.9723 - val_loss: 0.1548 - val_accuracy: 0.9540\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 00074: LearningRateScheduler setting learning rate to 3.377693875969219e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0873 - accuracy: 0.9725 - val_loss: 0.1548 - val_accuracy: 0.9539\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 00075: LearningRateScheduler setting learning rate to 3.0562638056478614e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0873 - accuracy: 0.9724 - val_loss: 0.1548 - val_accuracy: 0.9540\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 00076: LearningRateScheduler setting learning rate to 2.765421850739168e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0872 - accuracy: 0.9725 - val_loss: 0.1548 - val_accuracy: 0.9538\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 00077: LearningRateScheduler setting learning rate to 2.502257167203052e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0872 - accuracy: 0.9724 - val_loss: 0.1547 - val_accuracy: 0.9539\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 00078: LearningRateScheduler setting learning rate to 2.2641359144339846e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0872 - accuracy: 0.9725 - val_loss: 0.1547 - val_accuracy: 0.9542\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 00079: LearningRateScheduler setting learning rate to 2.048674894898932e-06.\n",
            "750/750 [==============================] - 4s 6ms/step - loss: 0.0871 - accuracy: 0.9725 - val_loss: 0.1547 - val_accuracy: 0.9539\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 00080: LearningRateScheduler setting learning rate to 1.853717702295441e-06.\n",
            "750/750 [==============================] - 4s 6ms/step - loss: 0.0871 - accuracy: 0.9725 - val_loss: 0.1548 - val_accuracy: 0.9542\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 00081: LearningRateScheduler setting learning rate to 1.6773131395125593e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0871 - accuracy: 0.9726 - val_loss: 0.1547 - val_accuracy: 0.9542\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 00082: LearningRateScheduler setting learning rate to 1.517695690394334e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0870 - accuracy: 0.9726 - val_loss: 0.1547 - val_accuracy: 0.9542\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 00083: LearningRateScheduler setting learning rate to 1.3732678498607102e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0870 - accuracy: 0.9725 - val_loss: 0.1547 - val_accuracy: 0.9542\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 00084: LearningRateScheduler setting learning rate to 1.2425841355397592e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0870 - accuracy: 0.9726 - val_loss: 0.1547 - val_accuracy: 0.9542\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 00085: LearningRateScheduler setting learning rate to 1.124336620894241e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0870 - accuracy: 0.9726 - val_loss: 0.1547 - val_accuracy: 0.9544\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 00086: LearningRateScheduler setting learning rate to 1.0173418450532208e-06.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0870 - accuracy: 0.9725 - val_loss: 0.1547 - val_accuracy: 0.9544\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 00087: LearningRateScheduler setting learning rate to 9.20528968337896e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0869 - accuracy: 0.9726 - val_loss: 0.1547 - val_accuracy: 0.9543\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 00088: LearningRateScheduler setting learning rate to 8.329290549381662e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0869 - accuracy: 0.9725 - val_loss: 0.1547 - val_accuracy: 0.9543\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 00089: LearningRateScheduler setting learning rate to 7.536653754773825e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0869 - accuracy: 0.9725 - val_loss: 0.1546 - val_accuracy: 0.9545\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 00090: LearningRateScheduler setting learning rate to 6.81944632410057e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0869 - accuracy: 0.9726 - val_loss: 0.1546 - val_accuracy: 0.9545\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 00091: LearningRateScheduler setting learning rate to 6.170490204333978e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0869 - accuracy: 0.9725 - val_loss: 0.1546 - val_accuracy: 0.9545\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 00092: LearningRateScheduler setting learning rate to 5.583290424505739e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0869 - accuracy: 0.9726 - val_loss: 0.1546 - val_accuracy: 0.9545\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 00093: LearningRateScheduler setting learning rate to 5.051970091854662e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0869 - accuracy: 0.9725 - val_loss: 0.1546 - val_accuracy: 0.9545\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 00094: LearningRateScheduler setting learning rate to 4.5712115739086634e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0869 - accuracy: 0.9726 - val_loss: 0.1546 - val_accuracy: 0.9545\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 00095: LearningRateScheduler setting learning rate to 4.1362032778316115e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0868 - accuracy: 0.9726 - val_loss: 0.1546 - val_accuracy: 0.9545\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 00096: LearningRateScheduler setting learning rate to 3.74259149438503e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0868 - accuracy: 0.9726 - val_loss: 0.1546 - val_accuracy: 0.9545\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 00097: LearningRateScheduler setting learning rate to 3.386436824542689e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0868 - accuracy: 0.9726 - val_loss: 0.1546 - val_accuracy: 0.9545\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 00098: LearningRateScheduler setting learning rate to 3.0641747526611013e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0868 - accuracy: 0.9726 - val_loss: 0.1546 - val_accuracy: 0.9545\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 00099: LearningRateScheduler setting learning rate to 2.772579971608847e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0868 - accuracy: 0.9726 - val_loss: 0.1546 - val_accuracy: 0.9545\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 00100: LearningRateScheduler setting learning rate to 2.508734102808764e-07.\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0868 - accuracy: 0.9726 - val_loss: 0.1546 - val_accuracy: 0.9545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEV6T0QK48ja"
      },
      "source": [
        "Step 9 - Use earlystoppping() when the desired metric has stopped improving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4nuGsbS5BzP",
        "outputId": "127c182a-f877-490b-ba19-6776f6c3446e"
      },
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0, patience=5, verbose=0,\n",
        "    mode='auto', baseline=None, restore_best_weights=False)\n",
        "\n",
        "model3 = create_model2()\n",
        "model3.compile(optimizer='sgd', # Learning rate defaults to 0.01\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "history = model3.fit(x_train, \n",
        "    Y_train, \n",
        "    epochs=800, \n",
        "    validation_split=0.2,\n",
        "    batch_size=64, callbacks=[callback])\n",
        "len(history.history['loss']) \n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/800\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='flatten_4_input'), name='flatten_4_input', description=\"created by layer 'flatten_4_input'\"), but it was called on an input with incompatible shape (64, 784).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='flatten_4_input'), name='flatten_4_input', description=\"created by layer 'flatten_4_input'\"), but it was called on an input with incompatible shape (64, 784).\n",
            "742/750 [============================>.] - ETA: 0s - loss: 0.6983 - accuracy: 0.8110WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='flatten_4_input'), name='flatten_4_input', description=\"created by layer 'flatten_4_input'\"), but it was called on an input with incompatible shape (None, 784).\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.6950 - accuracy: 0.8117 - val_loss: 0.3597 - val_accuracy: 0.9006\n",
            "Epoch 2/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3350 - accuracy: 0.9046 - val_loss: 0.2913 - val_accuracy: 0.9188\n",
            "Epoch 3/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.2735 - accuracy: 0.9213 - val_loss: 0.2553 - val_accuracy: 0.9262\n",
            "Epoch 4/800\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.2424 - accuracy: 0.9298 - val_loss: 0.2371 - val_accuracy: 0.9296\n",
            "Epoch 5/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.2213 - accuracy: 0.9358 - val_loss: 0.2138 - val_accuracy: 0.9381\n",
            "Epoch 6/800\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.2039 - accuracy: 0.9408 - val_loss: 0.2050 - val_accuracy: 0.9398\n",
            "Epoch 7/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1910 - accuracy: 0.9435 - val_loss: 0.1937 - val_accuracy: 0.9428\n",
            "Epoch 8/800\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.1820 - accuracy: 0.9466 - val_loss: 0.1873 - val_accuracy: 0.9473\n",
            "Epoch 9/800\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.1705 - accuracy: 0.9502 - val_loss: 0.1754 - val_accuracy: 0.9495\n",
            "Epoch 10/800\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.1599 - accuracy: 0.9534 - val_loss: 0.1762 - val_accuracy: 0.9505\n",
            "Epoch 11/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1573 - accuracy: 0.9542 - val_loss: 0.1747 - val_accuracy: 0.9489\n",
            "Epoch 12/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1480 - accuracy: 0.9568 - val_loss: 0.1650 - val_accuracy: 0.9534\n",
            "Epoch 13/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1391 - accuracy: 0.9596 - val_loss: 0.1643 - val_accuracy: 0.9522\n",
            "Epoch 14/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1390 - accuracy: 0.9590 - val_loss: 0.1574 - val_accuracy: 0.9536\n",
            "Epoch 15/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1325 - accuracy: 0.9611 - val_loss: 0.1610 - val_accuracy: 0.9533\n",
            "Epoch 16/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1295 - accuracy: 0.9609 - val_loss: 0.1554 - val_accuracy: 0.9525\n",
            "Epoch 17/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1230 - accuracy: 0.9637 - val_loss: 0.1510 - val_accuracy: 0.9552\n",
            "Epoch 18/800\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.1202 - accuracy: 0.9646 - val_loss: 0.1485 - val_accuracy: 0.9555\n",
            "Epoch 19/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1172 - accuracy: 0.9651 - val_loss: 0.1533 - val_accuracy: 0.9545\n",
            "Epoch 20/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1139 - accuracy: 0.9651 - val_loss: 0.1420 - val_accuracy: 0.9565\n",
            "Epoch 21/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1100 - accuracy: 0.9674 - val_loss: 0.1427 - val_accuracy: 0.9582\n",
            "Epoch 22/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1019 - accuracy: 0.9691 - val_loss: 0.1381 - val_accuracy: 0.9580\n",
            "Epoch 23/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1060 - accuracy: 0.9688 - val_loss: 0.1392 - val_accuracy: 0.9582\n",
            "Epoch 24/800\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.1037 - accuracy: 0.9692 - val_loss: 0.1428 - val_accuracy: 0.9569\n",
            "Epoch 25/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1026 - accuracy: 0.9695 - val_loss: 0.1324 - val_accuracy: 0.9608\n",
            "Epoch 26/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0985 - accuracy: 0.9706 - val_loss: 0.1391 - val_accuracy: 0.9572\n",
            "Epoch 27/800\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0937 - accuracy: 0.9719 - val_loss: 0.1405 - val_accuracy: 0.9553\n",
            "Epoch 28/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0988 - accuracy: 0.9698 - val_loss: 0.1374 - val_accuracy: 0.9572\n",
            "Epoch 29/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0979 - accuracy: 0.9703 - val_loss: 0.1368 - val_accuracy: 0.9588\n",
            "Epoch 30/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0945 - accuracy: 0.9705 - val_loss: 0.1350 - val_accuracy: 0.9600\n",
            "Epoch 31/800\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0986 - accuracy: 0.9708 - val_loss: 0.1354 - val_accuracy: 0.9581\n",
            "Epoch 32/800\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0949 - accuracy: 0.9712 - val_loss: 0.1282 - val_accuracy: 0.9610\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZlwWt36fMRl"
      },
      "source": [
        "**Early stopping function stopped at 32 Epoch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB10b2rbCgH_"
      },
      "source": [
        "Step 10 - create checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o7LcrwqCg8y",
        "outputId": "4d7031e9-731e-4d62-8469-155a76ff47e8"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model4 = create_model2()\n",
        "model4.compile(optimizer='sgd', # Learning rate defaults to 0.01\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "filepath=\"./checkpoint/weights-improvement-{epoch:02d}-{accuracy:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "history2 = model4.fit(x_train, \n",
        "    Y_train, \n",
        "    epochs=150, \n",
        "    validation_split=0.2,\n",
        "    batch_size=64, callbacks=[callbacks_list])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='flatten_5_input'), name='flatten_5_input', description=\"created by layer 'flatten_5_input'\"), but it was called on an input with incompatible shape (64, 784).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='flatten_5_input'), name='flatten_5_input', description=\"created by layer 'flatten_5_input'\"), but it was called on an input with incompatible shape (64, 784).\n",
            "743/750 [============================>.] - ETA: 0s - loss: 0.6880 - accuracy: 0.8154WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='flatten_5_input'), name='flatten_5_input', description=\"created by layer 'flatten_5_input'\"), but it was called on an input with incompatible shape (None, 784).\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.6855 - accuracy: 0.8161 - val_loss: 0.3638 - val_accuracy: 0.9013\n",
            "\n",
            "Epoch 00001: accuracy improved from -inf to 0.81606, saving model to ./checkpoint/weights-improvement-01-0.82.hdf5\n",
            "Epoch 2/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3334 - accuracy: 0.9058 - val_loss: 0.2840 - val_accuracy: 0.9192\n",
            "\n",
            "Epoch 00002: accuracy improved from 0.81606 to 0.90579, saving model to ./checkpoint/weights-improvement-02-0.91.hdf5\n",
            "Epoch 3/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.2733 - accuracy: 0.9218 - val_loss: 0.2502 - val_accuracy: 0.9278\n",
            "\n",
            "Epoch 00003: accuracy improved from 0.90579 to 0.92177, saving model to ./checkpoint/weights-improvement-03-0.92.hdf5\n",
            "Epoch 4/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.2425 - accuracy: 0.9305 - val_loss: 0.2235 - val_accuracy: 0.9367\n",
            "\n",
            "Epoch 00004: accuracy improved from 0.92177 to 0.93046, saving model to ./checkpoint/weights-improvement-04-0.93.hdf5\n",
            "Epoch 5/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.2188 - accuracy: 0.9379 - val_loss: 0.2085 - val_accuracy: 0.9410\n",
            "\n",
            "Epoch 00005: accuracy improved from 0.93046 to 0.93790, saving model to ./checkpoint/weights-improvement-05-0.94.hdf5\n",
            "Epoch 6/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.2025 - accuracy: 0.9415 - val_loss: 0.2013 - val_accuracy: 0.9417\n",
            "\n",
            "Epoch 00006: accuracy improved from 0.93790 to 0.94154, saving model to ./checkpoint/weights-improvement-06-0.94.hdf5\n",
            "Epoch 7/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1888 - accuracy: 0.9451 - val_loss: 0.1940 - val_accuracy: 0.9459\n",
            "\n",
            "Epoch 00007: accuracy improved from 0.94154 to 0.94512, saving model to ./checkpoint/weights-improvement-07-0.95.hdf5\n",
            "Epoch 8/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1761 - accuracy: 0.9480 - val_loss: 0.1796 - val_accuracy: 0.9484\n",
            "\n",
            "Epoch 00008: accuracy improved from 0.94512 to 0.94800, saving model to ./checkpoint/weights-improvement-08-0.95.hdf5\n",
            "Epoch 9/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1669 - accuracy: 0.9509 - val_loss: 0.1765 - val_accuracy: 0.9486\n",
            "\n",
            "Epoch 00009: accuracy improved from 0.94800 to 0.95094, saving model to ./checkpoint/weights-improvement-09-0.95.hdf5\n",
            "Epoch 10/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1595 - accuracy: 0.9536 - val_loss: 0.1684 - val_accuracy: 0.9524\n",
            "\n",
            "Epoch 00010: accuracy improved from 0.95094 to 0.95365, saving model to ./checkpoint/weights-improvement-10-0.95.hdf5\n",
            "Epoch 11/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1515 - accuracy: 0.9543 - val_loss: 0.1651 - val_accuracy: 0.9538\n",
            "\n",
            "Epoch 00011: accuracy improved from 0.95365 to 0.95431, saving model to ./checkpoint/weights-improvement-11-0.95.hdf5\n",
            "Epoch 12/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1407 - accuracy: 0.9588 - val_loss: 0.1605 - val_accuracy: 0.9526\n",
            "\n",
            "Epoch 00012: accuracy improved from 0.95431 to 0.95881, saving model to ./checkpoint/weights-improvement-12-0.96.hdf5\n",
            "Epoch 13/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1352 - accuracy: 0.9594 - val_loss: 0.1560 - val_accuracy: 0.9548\n",
            "\n",
            "Epoch 00013: accuracy improved from 0.95881 to 0.95942, saving model to ./checkpoint/weights-improvement-13-0.96.hdf5\n",
            "Epoch 14/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1339 - accuracy: 0.9606 - val_loss: 0.1536 - val_accuracy: 0.9575\n",
            "\n",
            "Epoch 00014: accuracy improved from 0.95942 to 0.96056, saving model to ./checkpoint/weights-improvement-14-0.96.hdf5\n",
            "Epoch 15/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1317 - accuracy: 0.9614 - val_loss: 0.1498 - val_accuracy: 0.9567\n",
            "\n",
            "Epoch 00015: accuracy improved from 0.96056 to 0.96142, saving model to ./checkpoint/weights-improvement-15-0.96.hdf5\n",
            "Epoch 16/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1244 - accuracy: 0.9622 - val_loss: 0.1509 - val_accuracy: 0.9561\n",
            "\n",
            "Epoch 00016: accuracy improved from 0.96142 to 0.96225, saving model to ./checkpoint/weights-improvement-16-0.96.hdf5\n",
            "Epoch 17/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1185 - accuracy: 0.9650 - val_loss: 0.1422 - val_accuracy: 0.9591\n",
            "\n",
            "Epoch 00017: accuracy improved from 0.96225 to 0.96504, saving model to ./checkpoint/weights-improvement-17-0.97.hdf5\n",
            "Epoch 18/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1141 - accuracy: 0.9662 - val_loss: 0.1447 - val_accuracy: 0.9563\n",
            "\n",
            "Epoch 00018: accuracy improved from 0.96504 to 0.96619, saving model to ./checkpoint/weights-improvement-18-0.97.hdf5\n",
            "Epoch 19/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.1110 - accuracy: 0.9666 - val_loss: 0.1473 - val_accuracy: 0.9574\n",
            "\n",
            "Epoch 00019: accuracy improved from 0.96619 to 0.96660, saving model to ./checkpoint/weights-improvement-19-0.97.hdf5\n",
            "Epoch 20/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1111 - accuracy: 0.9659 - val_loss: 0.1443 - val_accuracy: 0.9572\n",
            "\n",
            "Epoch 00020: accuracy did not improve from 0.96660\n",
            "Epoch 21/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1068 - accuracy: 0.9676 - val_loss: 0.1444 - val_accuracy: 0.9572\n",
            "\n",
            "Epoch 00021: accuracy improved from 0.96660 to 0.96760, saving model to ./checkpoint/weights-improvement-21-0.97.hdf5\n",
            "Epoch 22/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.1053 - accuracy: 0.9686 - val_loss: 0.1474 - val_accuracy: 0.9554\n",
            "\n",
            "Epoch 00022: accuracy improved from 0.96760 to 0.96856, saving model to ./checkpoint/weights-improvement-22-0.97.hdf5\n",
            "Epoch 23/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1042 - accuracy: 0.9690 - val_loss: 0.1418 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00023: accuracy improved from 0.96856 to 0.96900, saving model to ./checkpoint/weights-improvement-23-0.97.hdf5\n",
            "Epoch 24/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1039 - accuracy: 0.9686 - val_loss: 0.1349 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00024: accuracy did not improve from 0.96900\n",
            "Epoch 25/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0996 - accuracy: 0.9699 - val_loss: 0.1388 - val_accuracy: 0.9594\n",
            "\n",
            "Epoch 00025: accuracy improved from 0.96900 to 0.96987, saving model to ./checkpoint/weights-improvement-25-0.97.hdf5\n",
            "Epoch 26/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0999 - accuracy: 0.9699 - val_loss: 0.1344 - val_accuracy: 0.9595\n",
            "\n",
            "Epoch 00026: accuracy improved from 0.96987 to 0.96990, saving model to ./checkpoint/weights-improvement-26-0.97.hdf5\n",
            "Epoch 27/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0982 - accuracy: 0.9705 - val_loss: 0.1338 - val_accuracy: 0.9603\n",
            "\n",
            "Epoch 00027: accuracy improved from 0.96990 to 0.97048, saving model to ./checkpoint/weights-improvement-27-0.97.hdf5\n",
            "Epoch 28/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0941 - accuracy: 0.9708 - val_loss: 0.1321 - val_accuracy: 0.9598\n",
            "\n",
            "Epoch 00028: accuracy improved from 0.97048 to 0.97083, saving model to ./checkpoint/weights-improvement-28-0.97.hdf5\n",
            "Epoch 29/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0939 - accuracy: 0.9714 - val_loss: 0.1334 - val_accuracy: 0.9592\n",
            "\n",
            "Epoch 00029: accuracy improved from 0.97083 to 0.97137, saving model to ./checkpoint/weights-improvement-29-0.97.hdf5\n",
            "Epoch 30/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0914 - accuracy: 0.9710 - val_loss: 0.1293 - val_accuracy: 0.9610\n",
            "\n",
            "Epoch 00030: accuracy did not improve from 0.97137\n",
            "Epoch 31/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0886 - accuracy: 0.9729 - val_loss: 0.1332 - val_accuracy: 0.9596\n",
            "\n",
            "Epoch 00031: accuracy improved from 0.97137 to 0.97285, saving model to ./checkpoint/weights-improvement-31-0.97.hdf5\n",
            "Epoch 32/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0900 - accuracy: 0.9717 - val_loss: 0.1340 - val_accuracy: 0.9599\n",
            "\n",
            "Epoch 00032: accuracy did not improve from 0.97285\n",
            "Epoch 33/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0880 - accuracy: 0.9732 - val_loss: 0.1316 - val_accuracy: 0.9603\n",
            "\n",
            "Epoch 00033: accuracy improved from 0.97285 to 0.97321, saving model to ./checkpoint/weights-improvement-33-0.97.hdf5\n",
            "Epoch 34/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0874 - accuracy: 0.9736 - val_loss: 0.1333 - val_accuracy: 0.9592\n",
            "\n",
            "Epoch 00034: accuracy improved from 0.97321 to 0.97360, saving model to ./checkpoint/weights-improvement-34-0.97.hdf5\n",
            "Epoch 35/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0871 - accuracy: 0.9733 - val_loss: 0.1295 - val_accuracy: 0.9619\n",
            "\n",
            "Epoch 00035: accuracy did not improve from 0.97360\n",
            "Epoch 36/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0904 - accuracy: 0.9718 - val_loss: 0.1406 - val_accuracy: 0.9582\n",
            "\n",
            "Epoch 00036: accuracy did not improve from 0.97360\n",
            "Epoch 37/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0963 - accuracy: 0.9703 - val_loss: 0.1353 - val_accuracy: 0.9593\n",
            "\n",
            "Epoch 00037: accuracy did not improve from 0.97360\n",
            "Epoch 38/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0890 - accuracy: 0.9725 - val_loss: 0.1322 - val_accuracy: 0.9605\n",
            "\n",
            "Epoch 00038: accuracy did not improve from 0.97360\n",
            "Epoch 39/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0839 - accuracy: 0.9735 - val_loss: 0.1270 - val_accuracy: 0.9615\n",
            "\n",
            "Epoch 00039: accuracy did not improve from 0.97360\n",
            "Epoch 40/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0790 - accuracy: 0.9753 - val_loss: 0.1261 - val_accuracy: 0.9624\n",
            "\n",
            "Epoch 00040: accuracy improved from 0.97360 to 0.97527, saving model to ./checkpoint/weights-improvement-40-0.98.hdf5\n",
            "Epoch 41/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0835 - accuracy: 0.9736 - val_loss: 0.1274 - val_accuracy: 0.9609\n",
            "\n",
            "Epoch 00041: accuracy did not improve from 0.97527\n",
            "Epoch 42/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0819 - accuracy: 0.9747 - val_loss: 0.1229 - val_accuracy: 0.9628\n",
            "\n",
            "Epoch 00042: accuracy did not improve from 0.97527\n",
            "Epoch 43/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0818 - accuracy: 0.9746 - val_loss: 0.1212 - val_accuracy: 0.9640\n",
            "\n",
            "Epoch 00043: accuracy did not improve from 0.97527\n",
            "Epoch 44/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0798 - accuracy: 0.9743 - val_loss: 0.1198 - val_accuracy: 0.9621\n",
            "\n",
            "Epoch 00044: accuracy did not improve from 0.97527\n",
            "Epoch 45/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0767 - accuracy: 0.9762 - val_loss: 0.1250 - val_accuracy: 0.9604\n",
            "\n",
            "Epoch 00045: accuracy improved from 0.97527 to 0.97619, saving model to ./checkpoint/weights-improvement-45-0.98.hdf5\n",
            "Epoch 46/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0802 - accuracy: 0.9748 - val_loss: 0.1240 - val_accuracy: 0.9622\n",
            "\n",
            "Epoch 00046: accuracy did not improve from 0.97619\n",
            "Epoch 47/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0764 - accuracy: 0.9769 - val_loss: 0.1236 - val_accuracy: 0.9626\n",
            "\n",
            "Epoch 00047: accuracy improved from 0.97619 to 0.97685, saving model to ./checkpoint/weights-improvement-47-0.98.hdf5\n",
            "Epoch 48/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0765 - accuracy: 0.9755 - val_loss: 0.1298 - val_accuracy: 0.9617\n",
            "\n",
            "Epoch 00048: accuracy did not improve from 0.97685\n",
            "Epoch 49/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0729 - accuracy: 0.9774 - val_loss: 0.1206 - val_accuracy: 0.9643\n",
            "\n",
            "Epoch 00049: accuracy improved from 0.97685 to 0.97735, saving model to ./checkpoint/weights-improvement-49-0.98.hdf5\n",
            "Epoch 50/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0710 - accuracy: 0.9777 - val_loss: 0.1173 - val_accuracy: 0.9653\n",
            "\n",
            "Epoch 00050: accuracy improved from 0.97735 to 0.97773, saving model to ./checkpoint/weights-improvement-50-0.98.hdf5\n",
            "Epoch 51/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0678 - accuracy: 0.9784 - val_loss: 0.1213 - val_accuracy: 0.9640\n",
            "\n",
            "Epoch 00051: accuracy improved from 0.97773 to 0.97838, saving model to ./checkpoint/weights-improvement-51-0.98.hdf5\n",
            "Epoch 52/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0678 - accuracy: 0.9793 - val_loss: 0.1173 - val_accuracy: 0.9653\n",
            "\n",
            "Epoch 00052: accuracy improved from 0.97838 to 0.97933, saving model to ./checkpoint/weights-improvement-52-0.98.hdf5\n",
            "Epoch 53/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0682 - accuracy: 0.9790 - val_loss: 0.1112 - val_accuracy: 0.9657\n",
            "\n",
            "Epoch 00053: accuracy did not improve from 0.97933\n",
            "Epoch 54/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0666 - accuracy: 0.9791 - val_loss: 0.1162 - val_accuracy: 0.9673\n",
            "\n",
            "Epoch 00054: accuracy did not improve from 0.97933\n",
            "Epoch 55/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0689 - accuracy: 0.9790 - val_loss: 0.1170 - val_accuracy: 0.9648\n",
            "\n",
            "Epoch 00055: accuracy did not improve from 0.97933\n",
            "Epoch 56/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0684 - accuracy: 0.9788 - val_loss: 0.1155 - val_accuracy: 0.9660\n",
            "\n",
            "Epoch 00056: accuracy did not improve from 0.97933\n",
            "Epoch 57/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0670 - accuracy: 0.9791 - val_loss: 0.1167 - val_accuracy: 0.9663\n",
            "\n",
            "Epoch 00057: accuracy did not improve from 0.97933\n",
            "Epoch 58/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0621 - accuracy: 0.9805 - val_loss: 0.1207 - val_accuracy: 0.9645\n",
            "\n",
            "Epoch 00058: accuracy improved from 0.97933 to 0.98050, saving model to ./checkpoint/weights-improvement-58-0.98.hdf5\n",
            "Epoch 59/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0629 - accuracy: 0.9801 - val_loss: 0.1226 - val_accuracy: 0.9632\n",
            "\n",
            "Epoch 00059: accuracy did not improve from 0.98050\n",
            "Epoch 60/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0613 - accuracy: 0.9809 - val_loss: 0.1197 - val_accuracy: 0.9655\n",
            "\n",
            "Epoch 00060: accuracy improved from 0.98050 to 0.98085, saving model to ./checkpoint/weights-improvement-60-0.98.hdf5\n",
            "Epoch 61/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0638 - accuracy: 0.9795 - val_loss: 0.1229 - val_accuracy: 0.9633\n",
            "\n",
            "Epoch 00061: accuracy did not improve from 0.98085\n",
            "Epoch 62/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0640 - accuracy: 0.9797 - val_loss: 0.1214 - val_accuracy: 0.9642\n",
            "\n",
            "Epoch 00062: accuracy did not improve from 0.98085\n",
            "Epoch 63/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0655 - accuracy: 0.9793 - val_loss: 0.1202 - val_accuracy: 0.9633\n",
            "\n",
            "Epoch 00063: accuracy did not improve from 0.98085\n",
            "Epoch 64/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0625 - accuracy: 0.9806 - val_loss: 0.1191 - val_accuracy: 0.9646\n",
            "\n",
            "Epoch 00064: accuracy did not improve from 0.98085\n",
            "Epoch 65/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0614 - accuracy: 0.9803 - val_loss: 0.1176 - val_accuracy: 0.9653\n",
            "\n",
            "Epoch 00065: accuracy did not improve from 0.98085\n",
            "Epoch 66/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0571 - accuracy: 0.9822 - val_loss: 0.1164 - val_accuracy: 0.9647\n",
            "\n",
            "Epoch 00066: accuracy improved from 0.98085 to 0.98221, saving model to ./checkpoint/weights-improvement-66-0.98.hdf5\n",
            "Epoch 67/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0546 - accuracy: 0.9829 - val_loss: 0.1179 - val_accuracy: 0.9640\n",
            "\n",
            "Epoch 00067: accuracy improved from 0.98221 to 0.98285, saving model to ./checkpoint/weights-improvement-67-0.98.hdf5\n",
            "Epoch 68/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0529 - accuracy: 0.9834 - val_loss: 0.1143 - val_accuracy: 0.9637\n",
            "\n",
            "Epoch 00068: accuracy improved from 0.98285 to 0.98340, saving model to ./checkpoint/weights-improvement-68-0.98.hdf5\n",
            "Epoch 69/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0568 - accuracy: 0.9819 - val_loss: 0.1149 - val_accuracy: 0.9641\n",
            "\n",
            "Epoch 00069: accuracy did not improve from 0.98340\n",
            "Epoch 70/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0577 - accuracy: 0.9812 - val_loss: 0.1120 - val_accuracy: 0.9666\n",
            "\n",
            "Epoch 00070: accuracy did not improve from 0.98340\n",
            "Epoch 71/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0546 - accuracy: 0.9827 - val_loss: 0.1119 - val_accuracy: 0.9673\n",
            "\n",
            "Epoch 00071: accuracy did not improve from 0.98340\n",
            "Epoch 72/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0525 - accuracy: 0.9839 - val_loss: 0.1159 - val_accuracy: 0.9656\n",
            "\n",
            "Epoch 00072: accuracy improved from 0.98340 to 0.98394, saving model to ./checkpoint/weights-improvement-72-0.98.hdf5\n",
            "Epoch 73/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0514 - accuracy: 0.9841 - val_loss: 0.1119 - val_accuracy: 0.9676\n",
            "\n",
            "Epoch 00073: accuracy improved from 0.98394 to 0.98408, saving model to ./checkpoint/weights-improvement-73-0.98.hdf5\n",
            "Epoch 74/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0515 - accuracy: 0.9838 - val_loss: 0.1190 - val_accuracy: 0.9648\n",
            "\n",
            "Epoch 00074: accuracy did not improve from 0.98408\n",
            "Epoch 75/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0525 - accuracy: 0.9844 - val_loss: 0.1071 - val_accuracy: 0.9679\n",
            "\n",
            "Epoch 00075: accuracy improved from 0.98408 to 0.98442, saving model to ./checkpoint/weights-improvement-75-0.98.hdf5\n",
            "Epoch 76/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0524 - accuracy: 0.9835 - val_loss: 0.1156 - val_accuracy: 0.9655\n",
            "\n",
            "Epoch 00076: accuracy did not improve from 0.98442\n",
            "Epoch 77/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0534 - accuracy: 0.9835 - val_loss: 0.1151 - val_accuracy: 0.9644\n",
            "\n",
            "Epoch 00077: accuracy did not improve from 0.98442\n",
            "Epoch 78/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0552 - accuracy: 0.9825 - val_loss: 0.1184 - val_accuracy: 0.9648\n",
            "\n",
            "Epoch 00078: accuracy did not improve from 0.98442\n",
            "Epoch 79/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0557 - accuracy: 0.9819 - val_loss: 0.1124 - val_accuracy: 0.9658\n",
            "\n",
            "Epoch 00079: accuracy did not improve from 0.98442\n",
            "Epoch 80/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0578 - accuracy: 0.9820 - val_loss: 0.1230 - val_accuracy: 0.9643\n",
            "\n",
            "Epoch 00080: accuracy did not improve from 0.98442\n",
            "Epoch 81/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0584 - accuracy: 0.9808 - val_loss: 0.1193 - val_accuracy: 0.9657\n",
            "\n",
            "Epoch 00081: accuracy did not improve from 0.98442\n",
            "Epoch 82/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0547 - accuracy: 0.9822 - val_loss: 0.1178 - val_accuracy: 0.9647\n",
            "\n",
            "Epoch 00082: accuracy did not improve from 0.98442\n",
            "Epoch 83/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0530 - accuracy: 0.9828 - val_loss: 0.1166 - val_accuracy: 0.9649\n",
            "\n",
            "Epoch 00083: accuracy did not improve from 0.98442\n",
            "Epoch 84/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0576 - accuracy: 0.9818 - val_loss: 0.1115 - val_accuracy: 0.9657\n",
            "\n",
            "Epoch 00084: accuracy did not improve from 0.98442\n",
            "Epoch 85/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0543 - accuracy: 0.9821 - val_loss: 0.1169 - val_accuracy: 0.9654\n",
            "\n",
            "Epoch 00085: accuracy did not improve from 0.98442\n",
            "Epoch 86/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0544 - accuracy: 0.9822 - val_loss: 0.1160 - val_accuracy: 0.9648\n",
            "\n",
            "Epoch 00086: accuracy did not improve from 0.98442\n",
            "Epoch 87/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0507 - accuracy: 0.9844 - val_loss: 0.1141 - val_accuracy: 0.9655\n",
            "\n",
            "Epoch 00087: accuracy did not improve from 0.98442\n",
            "Epoch 88/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0490 - accuracy: 0.9848 - val_loss: 0.1135 - val_accuracy: 0.9668\n",
            "\n",
            "Epoch 00088: accuracy improved from 0.98442 to 0.98479, saving model to ./checkpoint/weights-improvement-88-0.98.hdf5\n",
            "Epoch 89/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0453 - accuracy: 0.9861 - val_loss: 0.1131 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00089: accuracy improved from 0.98479 to 0.98615, saving model to ./checkpoint/weights-improvement-89-0.99.hdf5\n",
            "Epoch 90/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0472 - accuracy: 0.9848 - val_loss: 0.1065 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00090: accuracy did not improve from 0.98615\n",
            "Epoch 91/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0488 - accuracy: 0.9841 - val_loss: 0.1171 - val_accuracy: 0.9647\n",
            "\n",
            "Epoch 00091: accuracy did not improve from 0.98615\n",
            "Epoch 92/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0491 - accuracy: 0.9839 - val_loss: 0.1190 - val_accuracy: 0.9654\n",
            "\n",
            "Epoch 00092: accuracy did not improve from 0.98615\n",
            "Epoch 93/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0488 - accuracy: 0.9846 - val_loss: 0.1163 - val_accuracy: 0.9647\n",
            "\n",
            "Epoch 00093: accuracy did not improve from 0.98615\n",
            "Epoch 94/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0476 - accuracy: 0.9847 - val_loss: 0.1143 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00094: accuracy did not improve from 0.98615\n",
            "Epoch 95/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0483 - accuracy: 0.9841 - val_loss: 0.1167 - val_accuracy: 0.9667\n",
            "\n",
            "Epoch 00095: accuracy did not improve from 0.98615\n",
            "Epoch 96/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0504 - accuracy: 0.9836 - val_loss: 0.1134 - val_accuracy: 0.9668\n",
            "\n",
            "Epoch 00096: accuracy did not improve from 0.98615\n",
            "Epoch 97/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0478 - accuracy: 0.9847 - val_loss: 0.1110 - val_accuracy: 0.9674\n",
            "\n",
            "Epoch 00097: accuracy did not improve from 0.98615\n",
            "Epoch 98/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0441 - accuracy: 0.9865 - val_loss: 0.1148 - val_accuracy: 0.9669\n",
            "\n",
            "Epoch 00098: accuracy improved from 0.98615 to 0.98648, saving model to ./checkpoint/weights-improvement-98-0.99.hdf5\n",
            "Epoch 99/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0467 - accuracy: 0.9847 - val_loss: 0.1161 - val_accuracy: 0.9657\n",
            "\n",
            "Epoch 00099: accuracy did not improve from 0.98648\n",
            "Epoch 100/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0454 - accuracy: 0.9854 - val_loss: 0.1106 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00100: accuracy did not improve from 0.98648\n",
            "Epoch 101/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0430 - accuracy: 0.9862 - val_loss: 0.1146 - val_accuracy: 0.9668\n",
            "\n",
            "Epoch 00101: accuracy did not improve from 0.98648\n",
            "Epoch 102/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0442 - accuracy: 0.9857 - val_loss: 0.1171 - val_accuracy: 0.9653\n",
            "\n",
            "Epoch 00102: accuracy did not improve from 0.98648\n",
            "Epoch 103/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0477 - accuracy: 0.9853 - val_loss: 0.1169 - val_accuracy: 0.9654\n",
            "\n",
            "Epoch 00103: accuracy did not improve from 0.98648\n",
            "Epoch 104/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0495 - accuracy: 0.9835 - val_loss: 0.1172 - val_accuracy: 0.9662\n",
            "\n",
            "Epoch 00104: accuracy did not improve from 0.98648\n",
            "Epoch 105/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0490 - accuracy: 0.9839 - val_loss: 0.1169 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00105: accuracy did not improve from 0.98648\n",
            "Epoch 106/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0484 - accuracy: 0.9846 - val_loss: 0.1177 - val_accuracy: 0.9661\n",
            "\n",
            "Epoch 00106: accuracy did not improve from 0.98648\n",
            "Epoch 107/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0457 - accuracy: 0.9847 - val_loss: 0.1104 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00107: accuracy did not improve from 0.98648\n",
            "Epoch 108/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0439 - accuracy: 0.9861 - val_loss: 0.1123 - val_accuracy: 0.9672\n",
            "\n",
            "Epoch 00108: accuracy did not improve from 0.98648\n",
            "Epoch 109/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0453 - accuracy: 0.9848 - val_loss: 0.1174 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00109: accuracy did not improve from 0.98648\n",
            "Epoch 110/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0438 - accuracy: 0.9857 - val_loss: 0.1196 - val_accuracy: 0.9670\n",
            "\n",
            "Epoch 00110: accuracy did not improve from 0.98648\n",
            "Epoch 111/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0432 - accuracy: 0.9862 - val_loss: 0.1162 - val_accuracy: 0.9676\n",
            "\n",
            "Epoch 00111: accuracy did not improve from 0.98648\n",
            "Epoch 112/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0456 - accuracy: 0.9855 - val_loss: 0.1145 - val_accuracy: 0.9670\n",
            "\n",
            "Epoch 00112: accuracy did not improve from 0.98648\n",
            "Epoch 113/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0416 - accuracy: 0.9868 - val_loss: 0.1180 - val_accuracy: 0.9663\n",
            "\n",
            "Epoch 00113: accuracy improved from 0.98648 to 0.98683, saving model to ./checkpoint/weights-improvement-113-0.99.hdf5\n",
            "Epoch 114/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0409 - accuracy: 0.9871 - val_loss: 0.1156 - val_accuracy: 0.9660\n",
            "\n",
            "Epoch 00114: accuracy improved from 0.98683 to 0.98708, saving model to ./checkpoint/weights-improvement-114-0.99.hdf5\n",
            "Epoch 115/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0404 - accuracy: 0.9868 - val_loss: 0.1135 - val_accuracy: 0.9673\n",
            "\n",
            "Epoch 00115: accuracy did not improve from 0.98708\n",
            "Epoch 116/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0373 - accuracy: 0.9883 - val_loss: 0.1104 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00116: accuracy improved from 0.98708 to 0.98829, saving model to ./checkpoint/weights-improvement-116-0.99.hdf5\n",
            "Epoch 117/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0392 - accuracy: 0.9872 - val_loss: 0.1140 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00117: accuracy did not improve from 0.98829\n",
            "Epoch 118/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0392 - accuracy: 0.9877 - val_loss: 0.1176 - val_accuracy: 0.9670\n",
            "\n",
            "Epoch 00118: accuracy did not improve from 0.98829\n",
            "Epoch 119/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0402 - accuracy: 0.9867 - val_loss: 0.1126 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00119: accuracy did not improve from 0.98829\n",
            "Epoch 120/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0418 - accuracy: 0.9867 - val_loss: 0.1151 - val_accuracy: 0.9677\n",
            "\n",
            "Epoch 00120: accuracy did not improve from 0.98829\n",
            "Epoch 121/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0395 - accuracy: 0.9876 - val_loss: 0.1138 - val_accuracy: 0.9677\n",
            "\n",
            "Epoch 00121: accuracy did not improve from 0.98829\n",
            "Epoch 122/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0389 - accuracy: 0.9879 - val_loss: 0.1110 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00122: accuracy did not improve from 0.98829\n",
            "Epoch 123/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0391 - accuracy: 0.9877 - val_loss: 0.1125 - val_accuracy: 0.9675\n",
            "\n",
            "Epoch 00123: accuracy did not improve from 0.98829\n",
            "Epoch 124/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0420 - accuracy: 0.9859 - val_loss: 0.1124 - val_accuracy: 0.9671\n",
            "\n",
            "Epoch 00124: accuracy did not improve from 0.98829\n",
            "Epoch 125/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0467 - accuracy: 0.9847 - val_loss: 0.1203 - val_accuracy: 0.9672\n",
            "\n",
            "Epoch 00125: accuracy did not improve from 0.98829\n",
            "Epoch 126/150\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0447 - accuracy: 0.9854 - val_loss: 0.1134 - val_accuracy: 0.9677\n",
            "\n",
            "Epoch 00126: accuracy did not improve from 0.98829\n",
            "Epoch 127/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0454 - accuracy: 0.9845 - val_loss: 0.1149 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00127: accuracy did not improve from 0.98829\n",
            "Epoch 128/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0404 - accuracy: 0.9865 - val_loss: 0.1187 - val_accuracy: 0.9676\n",
            "\n",
            "Epoch 00128: accuracy did not improve from 0.98829\n",
            "Epoch 129/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0382 - accuracy: 0.9881 - val_loss: 0.1175 - val_accuracy: 0.9661\n",
            "\n",
            "Epoch 00129: accuracy did not improve from 0.98829\n",
            "Epoch 130/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0412 - accuracy: 0.9871 - val_loss: 0.1127 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00130: accuracy did not improve from 0.98829\n",
            "Epoch 131/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0389 - accuracy: 0.9875 - val_loss: 0.1136 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00131: accuracy did not improve from 0.98829\n",
            "Epoch 132/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0386 - accuracy: 0.9876 - val_loss: 0.1130 - val_accuracy: 0.9676\n",
            "\n",
            "Epoch 00132: accuracy did not improve from 0.98829\n",
            "Epoch 133/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0388 - accuracy: 0.9870 - val_loss: 0.1095 - val_accuracy: 0.9693\n",
            "\n",
            "Epoch 00133: accuracy did not improve from 0.98829\n",
            "Epoch 134/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0385 - accuracy: 0.9876 - val_loss: 0.1185 - val_accuracy: 0.9664\n",
            "\n",
            "Epoch 00134: accuracy did not improve from 0.98829\n",
            "Epoch 135/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0387 - accuracy: 0.9871 - val_loss: 0.1187 - val_accuracy: 0.9668\n",
            "\n",
            "Epoch 00135: accuracy did not improve from 0.98829\n",
            "Epoch 136/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0398 - accuracy: 0.9871 - val_loss: 0.1149 - val_accuracy: 0.9664\n",
            "\n",
            "Epoch 00136: accuracy did not improve from 0.98829\n",
            "Epoch 137/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0371 - accuracy: 0.9873 - val_loss: 0.1098 - val_accuracy: 0.9687\n",
            "\n",
            "Epoch 00137: accuracy did not improve from 0.98829\n",
            "Epoch 138/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0381 - accuracy: 0.9879 - val_loss: 0.1157 - val_accuracy: 0.9663\n",
            "\n",
            "Epoch 00138: accuracy did not improve from 0.98829\n",
            "Epoch 139/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0404 - accuracy: 0.9872 - val_loss: 0.1186 - val_accuracy: 0.9677\n",
            "\n",
            "Epoch 00139: accuracy did not improve from 0.98829\n",
            "Epoch 140/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0373 - accuracy: 0.9881 - val_loss: 0.1131 - val_accuracy: 0.9669\n",
            "\n",
            "Epoch 00140: accuracy did not improve from 0.98829\n",
            "Epoch 141/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0374 - accuracy: 0.9876 - val_loss: 0.1187 - val_accuracy: 0.9657\n",
            "\n",
            "Epoch 00141: accuracy did not improve from 0.98829\n",
            "Epoch 142/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0371 - accuracy: 0.9879 - val_loss: 0.1130 - val_accuracy: 0.9672\n",
            "\n",
            "Epoch 00142: accuracy did not improve from 0.98829\n",
            "Epoch 143/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0382 - accuracy: 0.9876 - val_loss: 0.1147 - val_accuracy: 0.9662\n",
            "\n",
            "Epoch 00143: accuracy did not improve from 0.98829\n",
            "Epoch 144/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0409 - accuracy: 0.9861 - val_loss: 0.1274 - val_accuracy: 0.9644\n",
            "\n",
            "Epoch 00144: accuracy did not improve from 0.98829\n",
            "Epoch 145/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0408 - accuracy: 0.9865 - val_loss: 0.1140 - val_accuracy: 0.9674\n",
            "\n",
            "Epoch 00145: accuracy did not improve from 0.98829\n",
            "Epoch 146/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0352 - accuracy: 0.9880 - val_loss: 0.1105 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00146: accuracy did not improve from 0.98829\n",
            "Epoch 147/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0332 - accuracy: 0.9893 - val_loss: 0.1109 - val_accuracy: 0.9696\n",
            "\n",
            "Epoch 00147: accuracy improved from 0.98829 to 0.98931, saving model to ./checkpoint/weights-improvement-147-0.99.hdf5\n",
            "Epoch 148/150\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0336 - accuracy: 0.9893 - val_loss: 0.1117 - val_accuracy: 0.9701\n",
            "\n",
            "Epoch 00148: accuracy did not improve from 0.98931\n",
            "Epoch 149/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0347 - accuracy: 0.9888 - val_loss: 0.1150 - val_accuracy: 0.9668\n",
            "\n",
            "Epoch 00149: accuracy did not improve from 0.98931\n",
            "Epoch 150/150\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0361 - accuracy: 0.9880 - val_loss: 0.1179 - val_accuracy: 0.9676\n",
            "\n",
            "Epoch 00150: accuracy did not improve from 0.98931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ3IUT3MGoqu"
      },
      "source": [
        "Step 11 - report accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VjigFT0AO2J",
        "outputId": "c0885ec2-60cf-4fd6-c740-426777376bd1"
      },
      "source": [
        "# Score for default simple dense model\n",
        "scores = model.evaluate(x_test, Y_test)\n",
        "print('Accuracy: ',scores[1] * 100)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 2.3402 - accuracy: 0.1135\n",
            "Accuracy:  11.349999904632568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B971Ly9SLrLC",
        "outputId": "2a82e8f8-a06c-4fec-8daa-a8dedeff1427"
      },
      "source": [
        "# Score for model with adam optimizer\n",
        "\n",
        "scores = model_adam.evaluate(x_test, Y_test)\n",
        "print('Accuracy: ',scores[1] * 100)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1496 - accuracy: 0.9541\n",
            "Accuracy:  95.41000127792358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7efTeko9Orm4",
        "outputId": "e66e4a6c-ad2c-473f-990c-852a69962ddf"
      },
      "source": [
        "# Score for model with 100 epoch and 0.01 learning rate\n",
        "\n",
        "scores = model2.evaluate(x_test, Y_test)\n",
        "print('Accuracy: ',scores[1] * 100)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1423 - accuracy: 0.9574\n",
            "Accuracy:  95.74000239372253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wByLqBx6GsA5",
        "outputId": "1c3b2bf1-16f6-4e91-c0e6-3f4c9e4ca0fc"
      },
      "source": [
        "# Score for model with early stopper function\n",
        "\n",
        "scores = model3.evaluate(x_test, Y_test)\n",
        "print('Accuracy: ',scores[1] * 100)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1280 - accuracy: 0.9608\n",
            "Accuracy:  96.07999920845032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YibAvlf8LuNt",
        "outputId": "d075e1a1-0eb3-4030-df27-6d6ab22e93ed"
      },
      "source": [
        "# Score for model with checkpoint function\n",
        "\n",
        "scores = model4.evaluate(x_test, Y_test)\n",
        "print('Accuracy: ',scores[1] * 100)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1058 - accuracy: 0.9688\n",
            "Accuracy:  96.88000082969666\n"
          ]
        }
      ]
    }
  ]
}